{
    "docs": [
        {
            "location": "/",
            "text": "Welcome\n\u00b6\n\n\nSome welcome text should appear here.\n\n\nPerhaps the logos should be added here as well?",
            "title": "Home"
        },
        {
            "location": "/#welcome",
            "text": "Some welcome text should appear here.  Perhaps the logos should be added here as well?",
            "title": "Welcome"
        },
        {
            "location": "/trainers/trainers/",
            "text": "Dr. Dan Andrews\n Bioinformatics Fellow, John Curtin School of Medical Research, Australian National University, Canberra   \n\n\n \nMs. Katherine Champ\n Workshop Coordinator, Project Officer, Bioplatform Autralia Ltd.  \n\n\n \nDr. Zhiliang Chen\n Postdoctoral Research Associate, The University of New South Wales (UNSW), Sydney  \n\n\n \nDr. Susan Corley\n Postdoctoral Research Associate, The University of New South Wales (UNSW), Sydney  \n\n\n \nDr. Nandan Deshpande\n Postdoctoral Research Associate, The University of New South Wales (UNSW), Sydney  \n\n\n \nDr. Konsta Duesing\n Research Team Leader - Statistics & Bioinformatics, CSIRO Animal, Food and Health Science, Sydney  \n\n\n \nDr. Matthew Field\n Senior Research Fellow, Australian National University/James Cook University, Cairns\n\n\n \nDr. Velimir Gayevskiy\n Translational Bioinformatics Officer, KCCG, Garvan Institute of Medical Research NSW  \n\n\n \nPaul Greenfield\n Principal Experimental Scientist, CSIRO, Sydney  \n\n\n \nDr. Xi (Sean) Li\n The Australian National University, Canberra  \n\n\n \nDr. Annette McGrath\n Principal Research Scientist, Team Leader, Life Science Informatics DATA61, CSIRO, Canberra  \n\n\n \nMr. Sean McWilliam\n Bioinformatics Analyst, CSIRO Agriculture, Brisbane  \n\n\n \nDr. Philippe Moncuquet\n Research Project Officer, Cotton Disease Markers, CSIRO, Canberra\n\n\n \nDr. Paula Moolhuijzen\n Bioinformatics Analyst, Centre for Crop Disease Management, Curtin University, Perth  \n\n\n \nDr. Ann-Marie Patch\n Senior Research Officer, Medical Genomics QIMR Berghofer Medical Research Institute, Brisbane  \n\n\n \nDr. Gayle Philip\n Research Fellow (Bioinformatics), Melbourne Bioinformatics, Carlton, Melbourne  \n\n\n \nMr. Jerico Revote\n Software Developer Monash eResearch Centre, Monash University, Clayton Melbourne  \n\n\n \nA/Prof. Torsten Seemann\n Lead Bioinformatician, Melbourne Bioinformatics and MDU-PHL, The University of Melbourne, VIC  \n\n\n \nDr Anna Syme\n Bioinformatician, Melbourne Bioinformatics, Melbourne  \n\n\n \nDr. Erdahl Teber\n Senior Research Officer (Bioinformatics), Children Medical Research Institute, Kids Cancer Alliance, University of Sydney, Sydney  \n\n\n \nDr. Sonika Tyagi\n Bioinformatics Supervisor, Australian Genome Research Facility Ltd, The Walter and Eliza Hall Institute, Melbourne   \n\n\n \nDr. Nathan S. Watson-Haigh\n Research Fellow in Bioinformatics, The Australian Centre for Plant Functional Genomics (ACPFG), Adelaide",
            "title": "The Trainers"
        },
        {
            "location": "/timetables/timetable_longRead/",
            "text": "Long-read Data Analysis Workshop\n\u00b6\n\n\n\n\nSydney - 18\nth\n - 19\nth\n July 2017\n\n\nInstructors\n\u00b6\n\n\n\n\nKatherine Champ (KC) - Bioplatforms Australia, Sydney\n\n\nTonia Russell (TR) - Ramaciotti Centre for Genomics, Sydney\n\n\nAnna Syme (AS) - Melbourne Bioinformatics, Melbourne\n\n\nTorsten Seemann (TS) - Melbourne Bioinformatics, Melbourne\n\n\nNandan Deshpande - UNSW System Biology Initiative, Sydney\n\n\n\n\nTimetable\n\u00b6\n\n\nDay 1\n\u00b6\n\n\n18\nth\n July\n - \nRoom M020, The Red Centre, Kensington Campus, UNSW, Sydney\n\n\n\n\n\n\n\n\nTime\n\n\nTopic\n\n\nLinks\n\n\nInstructor\n\n\n\n\n\n\n\n\n\n\n9:00\n\n\nWelcome and registration\n\n\n\n\nWorkshop Host and KC\n\n\n\n\n\n\n9:30\n\n\nIntroduction to long-read sequencing (Lecture)\n\n\n\n\nTS\n\n\n\n\n\n\n10:30\n\n\nBreak\n\n\n\n\n\n\n\n\n\n\n10:30\n\n\nLong-read data \u2013 Practical considerations (Lecture)\n\n\n\n\nTR\n\n\n\n\n\n\n11:30\n\n\nIntroduction to Command-line (Practical)\n\n\n\n\nND\n\n\n\n\n\n\n13:00\n\n\nLunch\n\n\n\n\n\n\n\n\n\n\n14:00\n\n\nIntroduction to NGS \u2013 Technology, data formats and quality control\n\n\n\n\nND\n\n\n\n\n\n\n14:30\n\n\nIllumina de novo assembly (Velvet) (Practical)\n\n\n\n\nND\n\n\n\n\n\n\n15:00\n\n\nCoffee break\n\n\n\n\n\n\n\n\n\n\n15:30\n\n\nIllumina de novo assembly (Velvet) (Practical)\n\n\n\n\nND\n\n\n\n\n\n\n17:00\n\n\nQ&A\n\n\n\n\nAll\n\n\n\n\n\n\n\n\nDay 2\n\u00b6\n\n\n19\nth\n July\n - \nRoom M020, The Red Centre, Kensington Campus, UNSW, Sydney\n\n\n\n\n\n\n\n\nTime\n\n\nTopic\n\n\nLinks\n\n\nInstructor\n\n\n\n\n\n\n\n\n\n\n9:00\n\n\nLong-read data \u2013 Quality control (Practical)\n\n\n\n\nTR\n\n\n\n\n\n\n10:30\n\n\nCoffee break\n\n\n\n\n\n\n\n\n\n\n11:00\n\n\nPacBio assembly + Illimuna polishing (Practical)\n\n\n\n\nTS and AS\n\n\n\n\n\n\n12:30\n\n\nLunch\n\n\n\n\n\n\n\n\n\n\n13:30\n\n\nPacBio assembly + Illimuna polishing (Continue) (Practical)\n\n\n\n\nTS and AS\n\n\n\n\n\n\n15:00\n\n\nBreak\n\n\n\n\n\n\n\n\n\n\n15:30\n\n\nComparison of Illimuna and PacBio assembly\n\n\n\n\nTS\n\n\n\n\n\n\n16:45\n\n\nQ&A, wrap up (how to access course\u2019s VM) & survey\n\n\n\n\nAll",
            "title": "Metagenomics"
        },
        {
            "location": "/timetables/timetable_longRead/#long-read-data-analysis-workshop",
            "text": "Sydney - 18 th  - 19 th  July 2017",
            "title": "Long-read Data Analysis Workshop"
        },
        {
            "location": "/timetables/timetable_longRead/#instructors",
            "text": "Katherine Champ (KC) - Bioplatforms Australia, Sydney  Tonia Russell (TR) - Ramaciotti Centre for Genomics, Sydney  Anna Syme (AS) - Melbourne Bioinformatics, Melbourne  Torsten Seemann (TS) - Melbourne Bioinformatics, Melbourne  Nandan Deshpande - UNSW System Biology Initiative, Sydney",
            "title": "Instructors"
        },
        {
            "location": "/timetables/timetable_longRead/#timetable",
            "text": "",
            "title": "Timetable"
        },
        {
            "location": "/timetables/timetable_longRead/#day-1",
            "text": "18 th  July  -  Room M020, The Red Centre, Kensington Campus, UNSW, Sydney     Time  Topic  Links  Instructor      9:00  Welcome and registration   Workshop Host and KC    9:30  Introduction to long-read sequencing (Lecture)   TS    10:30  Break      10:30  Long-read data \u2013 Practical considerations (Lecture)   TR    11:30  Introduction to Command-line (Practical)   ND    13:00  Lunch      14:00  Introduction to NGS \u2013 Technology, data formats and quality control   ND    14:30  Illumina de novo assembly (Velvet) (Practical)   ND    15:00  Coffee break      15:30  Illumina de novo assembly (Velvet) (Practical)   ND    17:00  Q&A   All",
            "title": "Day 1"
        },
        {
            "location": "/timetables/timetable_longRead/#day-2",
            "text": "19 th  July  -  Room M020, The Red Centre, Kensington Campus, UNSW, Sydney     Time  Topic  Links  Instructor      9:00  Long-read data \u2013 Quality control (Practical)   TR    10:30  Coffee break      11:00  PacBio assembly + Illimuna polishing (Practical)   TS and AS    12:30  Lunch      13:30  PacBio assembly + Illimuna polishing (Continue) (Practical)   TS and AS    15:00  Break      15:30  Comparison of Illimuna and PacBio assembly   TS    16:45  Q&A, wrap up (how to access course\u2019s VM) & survey   All",
            "title": "Day 2"
        },
        {
            "location": "/preamble/",
            "text": "Providing Feedback\n\u00b6\n\n\nWhile we endeavour to deliver a workshop with quality content and\ndocumentation in a venue conducive to an exciting, well run hands-on\nworkshop with a bunch of knowledgeable and likable trainers, we know\nthere are things we could do better.\n\n\nWhilst we want to know what didn\u2019t quite hit the mark for you, what\nwould be most helpful and least depressing, would be for you to provide\nways to improve the workshop. i.e. constructive feedback. After all, if\nwe knew something wasn\u2019t going to work, we wouldn\u2019t have done it or put\nit into the workshop in the first place! Remember, we\u2019re experts in the\nfield of bioinformatics not experts in the field of biology!\n\n\nClearly, we also want to know what we did well! This gives us that \u201cfeel\ngood\u201d factor which will see us through those long days and nights in the\nlead up to such hands-on workshops!\n\n\nWith that in mind, we\u2019ll provide three really high tech mechanism\nthrough which you can provide anonymous feedback during the workshop:\n\n\n\n\n\n\nA sheet of paper, from a flip-chart, sporting a \u201chappy\u201d face and a\n\u201cnot so happy\u201d face. Armed with a stack of colourful post-it notes, your\nmission is to see how many comments you can stick on the \u201chappy\u201d side!\n\n\n\n\n\n\nSome empty ruled pages at the back of this handout. Use them for\nyour own personal notes or for write specific comments/feedback about\nthe workshop as it progresses.\n\n\n\n\n\n\nAn online post-workshop evaluation survey. We\u2019ll ask you to complete\nthis before you leave. If you\u2019ve used the blank pages at the back of\nthis handout to make feedback notes, you\u2019ll be able to provide more\nspecific and helpful feedback with the least amount of brain-drain!\n\n\n\n\n\n\nDocument Structure\n\u00b6\n\n\nWe have provided you with an electronic copy of the workshop\u2019s hands-on\ntutorial documents. We have done this for two reasons: 1) you will have\nsomething to take away with you at the end of the workshop, and 2) you\ncan save time (mis)typing commands on the command line by using\ncopy-and-paste.\n\n\n\n\nWhile you could fly through the hands-on sessions doing copy-and-paste\nyou will learn more if you take the time, saved from not having to type\nall those commands, to understand what each command is doing!\n\n\n\n\nThe commands to enter at a terminal look something like this:\n\n\n1\ntophat --solexa-quals -g 2 --library-type fr-unstranded -j annotation/Danio_rerio.Zv9.66.spliceSites -o tophat/ZV9_2cells genome/ZV9 data/2cells_1.fastq data/2cells_2.fastq\n\n\n\n\n\n\nThe following styled code is not to be entered at a terminal, it is\nsimply to show you the syntax of the command. You must use your own\njudgement to substitute in the correct arguments, options, filenames etc\n\n\n1\ntophat [options]* <index_base> <reads_1> <reads_2>\n\n\n\n\n\n\nThe following is an example how of R commands are styled:\n\n\n1\n2\n3\n4\n5\nR \n--\nno\n-\nsave\n\n\nlibrary\n(\nplotrix\n)\n\ndata \n<-\n read.table\n(\n\"run_25/stats.txt\"\n,\n header\n=\nTRUE\n)\n\nweighted.hist\n(\ndata\n$\nshort1_cov\n+\ndata\n$\nshort2_cov\n,\n data\n$\nlgth\n,\n breaks\n=\n0\n:\n70\n)\n\n\nq\n()\n\n\n\n\n\n\n\nThe following icons are used throughout the documentation\nto help you navigate around the document more easily:\n\n\n\n\nQuestion\n\n\nQuestions to answer.\n\n\n\n\n\n\nAnswer\n\n\nAnswers will be provided at the end of the workskop.\n\n\n\n\n\n\nImportant\n\n\nThis is important. \n\n\n\n\n\n\nSTOP\n\n\nWarning - STOP and read.\n\n\n\n\n\n\nBonus exercise\n\n\nBonus exercise for fast learners.\n\n\n\n\n\n\nAdvanced exercise\n\n\nAdvanced exercise for super-fast learners\n\n\n\n\nResources Used\n\u00b6\n\n\nWe have provided you with an environment which contains all the tools\nand data you need for the duration of this workshop. However, we also\nprovide details about the tools and data used by each module at the\nstart of the respective module documentation.",
            "title": "Workshop Information"
        },
        {
            "location": "/preamble/#providing-feedback",
            "text": "While we endeavour to deliver a workshop with quality content and\ndocumentation in a venue conducive to an exciting, well run hands-on\nworkshop with a bunch of knowledgeable and likable trainers, we know\nthere are things we could do better.  Whilst we want to know what didn\u2019t quite hit the mark for you, what\nwould be most helpful and least depressing, would be for you to provide\nways to improve the workshop. i.e. constructive feedback. After all, if\nwe knew something wasn\u2019t going to work, we wouldn\u2019t have done it or put\nit into the workshop in the first place! Remember, we\u2019re experts in the\nfield of bioinformatics not experts in the field of biology!  Clearly, we also want to know what we did well! This gives us that \u201cfeel\ngood\u201d factor which will see us through those long days and nights in the\nlead up to such hands-on workshops!  With that in mind, we\u2019ll provide three really high tech mechanism\nthrough which you can provide anonymous feedback during the workshop:    A sheet of paper, from a flip-chart, sporting a \u201chappy\u201d face and a\n\u201cnot so happy\u201d face. Armed with a stack of colourful post-it notes, your\nmission is to see how many comments you can stick on the \u201chappy\u201d side!    Some empty ruled pages at the back of this handout. Use them for\nyour own personal notes or for write specific comments/feedback about\nthe workshop as it progresses.    An online post-workshop evaluation survey. We\u2019ll ask you to complete\nthis before you leave. If you\u2019ve used the blank pages at the back of\nthis handout to make feedback notes, you\u2019ll be able to provide more\nspecific and helpful feedback with the least amount of brain-drain!",
            "title": "Providing Feedback"
        },
        {
            "location": "/preamble/#document-structure",
            "text": "We have provided you with an electronic copy of the workshop\u2019s hands-on\ntutorial documents. We have done this for two reasons: 1) you will have\nsomething to take away with you at the end of the workshop, and 2) you\ncan save time (mis)typing commands on the command line by using\ncopy-and-paste.   While you could fly through the hands-on sessions doing copy-and-paste\nyou will learn more if you take the time, saved from not having to type\nall those commands, to understand what each command is doing!   The commands to enter at a terminal look something like this:  1 tophat --solexa-quals -g 2 --library-type fr-unstranded -j annotation/Danio_rerio.Zv9.66.spliceSites -o tophat/ZV9_2cells genome/ZV9 data/2cells_1.fastq data/2cells_2.fastq   The following styled code is not to be entered at a terminal, it is\nsimply to show you the syntax of the command. You must use your own\njudgement to substitute in the correct arguments, options, filenames etc  1 tophat [options]* <index_base> <reads_1> <reads_2>   The following is an example how of R commands are styled:  1\n2\n3\n4\n5 R  -- no - save  library ( plotrix ) \ndata  <-  read.table ( \"run_25/stats.txt\" ,  header = TRUE ) \nweighted.hist ( data $ short1_cov + data $ short2_cov ,  data $ lgth ,  breaks = 0 : 70 )  q ()    The following icons are used throughout the documentation\nto help you navigate around the document more easily:   Question  Questions to answer.    Answer  Answers will be provided at the end of the workskop.    Important  This is important.     STOP  Warning - STOP and read.    Bonus exercise  Bonus exercise for fast learners.    Advanced exercise  Advanced exercise for super-fast learners",
            "title": "Document Structure"
        },
        {
            "location": "/preamble/#resources-used",
            "text": "We have provided you with an environment which contains all the tools\nand data you need for the duration of this workshop. However, we also\nprovide details about the tools and data used by each module at the\nstart of the respective module documentation.",
            "title": "Resources Used"
        },
        {
            "location": "/modules/metagenomics-module-qc/ngs-qc/",
            "text": "Data Quality Control\n\u00b6\n\n\nKey Learning Outcomes\n\u00b6\n\n\n\n\nAfter completing this practical the trainee should be able to:\n\n\n\n\n\n\nAssess the overall quality of NGS (FastQ format) sequence reads\n\n\n\n\n\n\nVisualise the quality, and other associated matrices, of reads to decide on filters and cutoffs for cleaning up data ready for downstream analysis\n\n\n\n\n\n\nClean up adaptors and pre-process the sequence data for further analysis\n\n\n\n\n\n\nResources You\u2019ll be Using\n\u00b6\n\n\n\n\nTools Used\n\u00b6\n\n\nFastQC:\n\n\nhttp://www.bioinformatics.babraham.ac.uk/projects/fastqc/\n\n\nSkewer:\n\n\nhttp://sourceforge.net/projects/skewer/\n\n\nFASTX-Toolkit:\n\n\nhttp://hannonlab.cshl.edu/fastx_toolkit/\n>\n\n\nPicard:\n\n\nhttp://picard.sourceforge.net/\n\n\nUseful Links\n\u00b6\n\n\n\n\nFASTQ Encoding:\n\n\nhttp://en.wikipedia.org/wiki/FASTQ_format#Encoding\n\n\nIntroduction\n\u00b6\n\n\n\n\nGoing on a blind date with your read set? For a better understanding of\nthe consequences please check the data quality!\n\n\nFor the purpose of this tutorial we are focusing only on Illumina\nsequencing which uses \u2019sequence by synthesis\u2019 technology in a highly\nparallel fashion. Although Illumina high throughput sequencing provides\nhighly accurate sequence data, several sequence artifacts, including\nbase calling errors and small insertions/deletions, poor quality reads\nand primer/adapter contamination are quite common in the high throughput\nsequencing data. The primary errors are substitution errors. The error\nrates can vary from 0.5-2.0% with errors mainly rising in frequency at\nthe 3\u2019 ends of reads.\n\n\nOne way to investigate sequence data quality is to visualize the quality\nscores and other metrics in a compact manner to get an idea about the\nquality of a read data set. Read data sets can be improved by pre\nprocessing in different ways like trimming off low quality bases,\ncleaning up any sequencing adapters, removing PCR duplicates and\nscreening for contamination. We can also look at other statistics such\nas, sequence length distribution, base composition, sequence complexity,\npresence of ambiguous bases etc. to assess the overall quality of the\ndata set.\n\n\nHighly redundant coverage (>15X) of the genome can be used to correct\nsequencing errors in the reads before assembly. Various k-mer based\nerror correction methods exist but are beyond the scope of this\ntutorial.\n\n\nQuality Value Encoding Schema\n\u00b6\n\n\nIn order to use a single character to encode Phred qualities, ASCII\ncharacters are used\n(\nhttp://shop.alterlinks.com/ascii-table/ascii-table-us.php\n). All ASCII\ncharacters have a decimal number associated with them but the first 32\ncharacters are non-printable (e.g. backspace, shift, return, escape).\nTherefore, the first printable ASCII character is number 33, the\nexclamation mark (!). In Phred+33 encoded quality values the exclamation\nmark takes the Phred quality score of zero.\n\n\nEarly Solexa (now Illumina) sequencing needed to encode negative quality\nvalues. Because ASCII characters $<$ 33 are non-printable, using the\nPhred+33 encoding was not possible. Therefore, they simply moved the\noffset from 33 to 64 thus inventing the Phred+64 encoded quality values.\nIn this encoding a Phred quality of zero is denoted by the ASCII number\n64 (the @ character). Since Illumina 1.8, quality values are now encoded\nusing Phred+33.\n\n\nFASTQ does not provide a way to describe what quality encoding is used\nfor the quality values. Therefore, you should find this out from your\nsequencing provider. Alternatively, you may be able to figure this out\nby determining what ASCII characters are present in the FASTQ file. E.g\nthe presence of numbers in the quality strings, can only mean the\nquality values are Phred+33 encoded. However, due to the overlapping\nnature of the Phred+33 and Phred+64 encoding schema it is not always\npossible to identify what encoding is in use. For example, if the only\ncharacters seen in the quality string are (\n@ABCDEFGHI\n), then it is\nimpossible to know if you have really good Phred+33 encoded qualities or\nreally bad Phred+64 encoded qualities.\n\n\nFor a graphical representation of the different ASCII characters used in\nthe two encoding schema see:\n\nhttp://en.wikipedia.org/wiki/FASTQ_format#Encoding\n\n\nPrepare the Environment\n\u00b6\n\n\n\n\nTo investigate sequence data quality we will demonstrate tools called\nFastQC and Skewer. FastQC will process and present the reports in a\nvisual manner. Based on the results, the sequence data can be processed\nusing the Skewer. We will use one data set in this practical, which can\nbe found in the QC directory on your desktop.\n\n\nOpen the Terminal and go to the directory where the data are stored:\n\n\n1\n2\n3\nls\ncd qc\npwd\n\n\n\n\n\n\nAt any time, help can be displayed for FastQC using the following\ncommand:\n\n\n1\nfastqc -h\n\n\n\n\n\n\nLook at SYNOPSIS (Usage) and options after typing fastqc -h\n\n\nQuality Visualisation\n\u00b6\n\n\n\n\nWe have a file for a good quality and bad quality statistics. FastQC\ngenerates results in the form of a zipped and unzipped directory for\neach input file.\n\n\nExecute the following command on the two files:\n\n\n1\n2\nfastqc -f qcdemo_R1.fastq.gz\nfastqc -f qcdemo_R2.fastq.gz\n\n\n\n\n\n\nView the FastQC report file of the bad data using a web browser such as\nfirefox. The \u2019&\u2019 sign puts the job in the background.\n\n\n1\nfirefox qcdemo_R2_fastqc.html &\n\n\n\n\n\n\nThe report file will have a Basic Statistics table and various graphs\nand tables for different quality statistics. E.g.:\n\n\n\n\n\n\n\n\nFilename\n\n\nqcdemo_R2.fastq.gz\n\n\n\n\n\n\n\n\n\n\nFile type\n\n\nConventional base calls\n\n\n\n\n\n\nEncoding\n\n\nSanger / Illumina 1.9\n\n\n\n\n\n\nTotal Sequences\n\n\n1000000\n\n\n\n\n\n\nFiltered Sequences\n\n\n0\n\n\n\n\n\n\nSequence length\n\n\n150\n\n\n\n\n\n\n%GC\n\n\n37\n\n\n\n\n\n\n\n\nFastQC Basic Statistics table\n\n\n\n\nPer base sequence quality plot for\n\nqcdemo_R2.fastq.gz\n.\n\n\n\n\nA Phred quality score (or Q-score) expresses an error probability. In\nparticular, it serves as a convenient and compact way to communicate\nvery small error probabilities. The probability that base \nA\n is wrong \nP(sim A)\n is expressed by a quality score, \nQ(A)\n, according to the\nrelationship:\n\n\nQ(A) =-10 log10(P(sim A))\n\n\nThe relationship between the quality score and error probability is\ndemonstrated with the following table:\n\n\n\n\n\n\n\n\nQuality score\n\n\nError probability\n\n\nAccuracy of the base call\n\n\n\n\n\n\n\n\n\n\n10\n\n\n0.1\n\n\n90%\n\n\n\n\n\n\n20\n\n\n0.01\n\n\n99%\n\n\n\n\n\n\n30\n\n\n0.001\n\n\n99.9%\n\n\n\n\n\n\n40\n\n\n0.0001\n\n\n99.99%\n\n\n\n\n\n\n50\n\n\n0.00001\n\n\n99.999%\n\n\n\n\n\n\n\n\nError probabilities associated with various quality (Q) values\n\n\n[tab:quality_error_probs]\n\n\n\n\n\n\nHow many sequences were there in your file? What is the read length?\n\n\n\n\nThis is a spoiler: {%s%}Hello World.{%ends%}\n\n\n1\n> 1,000,000. read length=150bp\n\n\n\n\n\n\n\n\n\n\nDoes the quality score values vary throughout the read length? (hint:\nlook at the \u2019per base sequence quality plot\u2019)\n\n\n\n\nYes. Quality scores are dropping towards the end of the reads.\n\n\n\n\n\n\n\n\nWhat is the quality score range you see?\n\n\n\n\n2-40\n\n\n\n\n\n\n\n\nAt around which position do the scores start falling below Q20 for the 25% quartile range (25%of reads below Q20)?\n\n\n\n\nAround 30 bp position\n\n\n\n\n\n\n\n\nHow can we trim the reads to filter out the low quality data?\n\n\n\n\nBy trimming off the bases after a fixed position of the read or by trimming off bases based on the quality score.\n\n\n\n\n\n\n\n\nGood Quality Data\n\u00b6\n\n\nView the FastQC report files \nfastqc_report.html\n to see examples of a\ngood quality data and compare the quality plot with that of the\n\nbad_example_fastqc\n.\n\n\n1\nfirefox qcdemo_R1_fastqc.html &\n\n\n\n\n\n\nSequencing errors can complicate the downstream analysis, which normally\nrequires that reads be aligned to each other (for genome assembly) or to\na reference genome (for detection of mutations). Sequence reads\ncontaining errors may lead to ambiguous paths in the assembly or\nimproper gaps. In variant analysis projects sequence reads are aligned\nagainst the reference genome. The errors in the reads may lead to more\nmismatches than expected from mutations alone. But if these errors can\nbe removed or corrected, the read alignments and hence the variant\ndetection will improve. The assemblies will also improve after\npre-processing the reads to remove errors.\n\n\nRead Trimming\n\u00b6\n\n\n\n\nRead trimming can be done in a variety of different ways. Choose a\nmethod which best suits your data. Here we are giving examples of\nfixed-length trimming and quality-based trimming.\n\n\nQuality Based Trimming\n\u00b6\n\n\nBase call quality scores can be used to dynamically determine the trim\npoints for each read. A quality score threshold and minimum read length\nfollowing trimming can be used to remove low quality data.\n\n\nThe previous FastQC results show R1 is fine but R2 has low quality at\nthe end. There is no adaptor contamination though. We will be using\nSkewer to perform the quality trimming.\n\n\nRun the following command to quality trim a set of paired end data.\n\n\n1\n2\ncd\n ~/qc\nskewer -t \n20\n -l \n50\n  -q \n30\n -Q \n25\n -m pe qcdemo_R1.fastq.gz qcdemo_R2.fastq.gz\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n-t :   number of threads to use\n\n-l :   min length to keep after trimming\n\n-q :   Quality threshold used for trimming at 3\u2019 end\n\n-Q :   mean quality threshold for a read\n\n-m :   pair-end mode\n\n\n\n\n\n\nRun FastQC on the quality trimmed file and visualise the quality scores.\n\n\n1\n2\n3\n4\nfastqc -f fastq qcdemo_R1.fastq-trimmed-pair1.fastq\nfastqc -f fastq qcdemo_R1.fastq-trimmed-pair2.fastq\nfirefox qcdemo_R1.fastq-trimmed-pair1_fastqc.html &\nfirefox qcdemo_R1.fastq-trimmed-pair2_fastqc.html&\n\n\n\n\n\n\nLet\u2019s look at the quality from the second reads. The output should look\nlike:\n\n\nFastQC Basic Statistics table\n\n\n\n\n\n\n\n\nFilename\n\n\nqcdemo_R1.fastq-trimmed-pair2.fastq\n\n\n\n\n\n\n\n\n\n\nFile type\n\n\nConventional base calls\n\n\n\n\n\n\nEncoding\n\n\nSanger / Illumina 1.9\n\n\n\n\n\n\nTotal Sequences\n\n\n742262\n\n\n\n\n\n\nFiltered Sequences\n\n\n0\n\n\n\n\n\n\nSequence length\n\n\n50\n\n\n\n\n\n\n%GC\n\n\n37\n\n\n\n\n\n\n\n\nPer base sequence quality plot for the quality-trimmed\n\n\nqcdemo_R2.fastq.gz\n\n\n\n\n\n\n\n\nDid the number of total reads in R1 and R2 change after trimming?\n\n\n\n\n\n\nQuality trimming discarded >1000 reads. However, We retain a lot of maximal length reads which have good quality all the way to the ends.\n\n\n\n\n\n\nWhat reads lengths were obtained after quality based trimming?\n\n\n\n\n\n\n50-150 Reads <50 bp, following quality trimming, were discarded.\n\n\n\n\n\n\nDid you observe adapter sequences in the data?\n\n\n\n\n\n\nNo. (Hint: look at the overrepresented sequences.\n\n\n\n\n\n\nHow can you use -a option with fastqc ? (Hint: try fastqc -h).\n\n\n\n\n\n\nAdaptors can be supplied in a file for screening.\n\n\n\n\nAdapter Clipping\n\u00b6\n\n\nSometimes sequence reads may end up getting the leftover of adapters and\nprimers used in the sequencing process. It\u2019s good practice to screen\nyour data for these possible contamination for more sensitive alignment\nand assembly based analysis.\n\n\nThis is particularly important when read lengths can be longer than the\nmolecules being sequenced. For example when sequencing miRNAs.\n\n\nVarious QC tools are available to screen and/or clip these\nadapter/primer sequences from your data. Apart from skewer which will be\nusing today the following two tools are also useful for trimming and\nremoving adapter sequence.\n\n\nCutadapt: (\nhttp://code.google.com/p/cutadapt/\n)\nTrimmomatic:\n(\nhttp://www.usadellab.org/cms/?page=trimmomatic\n)\n\n\nHere we are demonstrating \nSkewer\n to trim a given adapter sequence.\n\n\n1\n2\n3\ncd ~/qc\nfastqc -f fastq  adaptorQC.fastq.gz\nskewer -x TGGAATTCTCGGGTGCCAAGGT -t 20 -l 10 -L 35 -q 30 adaptorQC.fastq.gz\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n-x :   adaptor sequence used\n\n-t :   number of threads to use\n\n-l :   min length to keep after trimming\n\n-L :   Max length to keep after trimming, in this experiment we were\n    expecting only small RNA fragments\n\n-Q :   Quality threshold used for trimming at 3\u2019 end. Use -m option to\n    control the end you want to trim\n\n\n\n\n\n\nRun FastQC on the adapter trimmed file and visualise the quality scores.\nFastqc now shows adaptor free results.\n\n\n1\n2\nfastqc -f fastq adaptorQC.fastq-trimmed.fastq\nfirefox adaptorQC.fastq-trimmed_fastqc.html &\n\n\n\n\n\n\nAn alternative tool, not installed on this system, for adapter clipping\nis \nfastq-mcf\n. A list of adapters is provided in a text file. For more\ninformation, see FastqMcf at\n\nhttp://code.google.com/p/ea-utils/wiki/FastqMcf\n.\n\n\nFixed Length Trimming\n\u00b6\n\n\nWe will not cover Fixed Length Trimming but provide the following for your information.\n Low quality read ends can be trimmed using a\nfixed-length trimming. We will use the \nfastx_trimmer\n from the\nFASTX-Toolkit. Usage message to find out various options you can use\nwith this tool. Type \nfastx_trimmer -h\n at anytime to display help.\n\n\nWe will now do fixed-length trimming of the \nbad_example.fastq\n file\nusing the following command. You should still be in the qc directory, if\nnot cd back in.\n\n\n1\n2\n3\n4\ncd ~/qc\nfastqc -f fastq bad_example.fastq\nfastx_trimmer -h\nfastx_trimmer -Q 33 -f 1 -l 80 -i bad_example.fastq -o bad_example_trimmed01.fastq\n\n\n\n\n\n\nWe used the following options in the command above:\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n-Q 33 :   Indicates the input quality scores are Phred+33 encoded\n\n-f :   First base to be retained in the output\n\n-l :   Last base to be retained in the output\n\n-i :   Input FASTQ file name\n\n-o :   Output file name\n\n\n\n\n\nRun FastQC on the trimmed file and visualise the quality scores of the\ntrimmed file.\n\n\n1\n2\nfastqc -f fastq bad_example_trimmed01.fastq\nfirefox bad_example_trimmed01_fastqc.html &\n\n\n\n\n\n\nThe output should look like:\n\n\n\n\n\n\n\n\nFilename\n\n\nbad_example_trimmed01.fastq\n\n\n\n\n\n\n\n\n\n\nFile type\n\n\nConventional base call\n\n\n\n\n\n\nEncoding\n\n\nSanger / Illumina 1.9\n\n\n\n\n\n\nTotal Sequences\n\n\n40000\n\n\n\n\n\n\nFiltered Sequences\n\n\n0\n\n\n\n\n\n\nSequence length\n\n\n80\n\n\n\n\n\n\n%GC\n\n\n48\n\n\n\n\n\n\n\n\n: FastQC Basic Statistics table\n\n\n\n\ntab:badexampletrimmed\n\n\n\n\n![Per base sequence quality plot for the fixed-length trimmed\n\nbad_example.fastq\n\n\n\n\nWhat values would you use for \n-f\n if you wanted to trim off 10 bases at\nthe 5\u2019 end of the reads?\n\n\n-f 11\n\n\nRemoving Duplicates\n\u00b6\n\n\nDuplicate reads are the ones having the same start and end coordinates.\nThis may be the result of technical duplication (too many PCR cycles),\nor over-sequencing (very high fold coverage). It is very important to\nput the duplication level in context of your experiment. For example,\nduplication level in targeted or re-sequencing projects may mean\nsomething different in RNA-seq experiments. In RNA-seq experiments\noversequencing is usually necessary when detecting low abundance\ntranscripts.\n\n\nThe duplication level computed by FastQC is based on sequence identity\nat the end of reads. Another tool, Picard, determines duplicates based\non identical start and end positions in SAM/BAM alignment files.\n\n\nWe will not cover Picard but provide the following for your\ninformation.\n\n\nPicard is a suite of tools for performing many common tasks with SAM/BAM\nformat files. For more information see the Picard website and\ninformation about the various command-line tools available:\n\n\nhttp://picard.sourceforge.net/command-line-overview.shtml\n\n\nA good list of tools for filtering PCR duplication can also be found at\n\nhttp://omictools.com/duplicate-reads-removal-c495-p1.html\n\n\nPicard is installed on this system in \n/usr/share/java\n\n\nOne of the Picard tools (MarkDuplicates) can be used to analyse and\nremove duplicates from the raw sequence data. The input for Picard is a\nsorted alignment file in BAM format. Short read aligners such as,\nbowtie, BWA and tophat can be used to align FASTQ files against a\nreference genome to generate SAM/BAM alignment format.\n\n\nInterested users can use the following general command to run the\nMarkDuplicates tool at their leisure. You only need to provide a BAM\nfile for the INPUT argument (not provided):\n\n\n1\n2\ncd ~/qc\njava -jar /usr/share/java/MarkDuplicates.jar INPUT=<alignment_file.bam> VALIDATION_STRINGENCY=LENIENT OUTPUT=alignment_file.dup METRICS_FILE=alignment_file.matric ASSUME_SORTED=true REMOVE_DUPLICATES=true",
            "title": "Metagenomics QC"
        },
        {
            "location": "/modules/metagenomics-module-qc/ngs-qc/#data-quality-control",
            "text": "",
            "title": "Data Quality Control"
        },
        {
            "location": "/modules/metagenomics-module-qc/ngs-qc/#key-learning-outcomes",
            "text": "After completing this practical the trainee should be able to:    Assess the overall quality of NGS (FastQ format) sequence reads    Visualise the quality, and other associated matrices, of reads to decide on filters and cutoffs for cleaning up data ready for downstream analysis    Clean up adaptors and pre-process the sequence data for further analysis",
            "title": "Key Learning Outcomes"
        },
        {
            "location": "/modules/metagenomics-module-qc/ngs-qc/#resources-youll-be-using",
            "text": "",
            "title": "Resources You\u2019ll be Using"
        },
        {
            "location": "/modules/metagenomics-module-qc/ngs-qc/#tools-used",
            "text": "FastQC:  http://www.bioinformatics.babraham.ac.uk/projects/fastqc/  Skewer:  http://sourceforge.net/projects/skewer/  FASTX-Toolkit:  http://hannonlab.cshl.edu/fastx_toolkit/ >  Picard:  http://picard.sourceforge.net/",
            "title": "Tools Used"
        },
        {
            "location": "/modules/metagenomics-module-qc/ngs-qc/#useful-links",
            "text": "FASTQ Encoding:  http://en.wikipedia.org/wiki/FASTQ_format#Encoding",
            "title": "Useful Links"
        },
        {
            "location": "/modules/metagenomics-module-qc/ngs-qc/#introduction",
            "text": "Going on a blind date with your read set? For a better understanding of\nthe consequences please check the data quality!  For the purpose of this tutorial we are focusing only on Illumina\nsequencing which uses \u2019sequence by synthesis\u2019 technology in a highly\nparallel fashion. Although Illumina high throughput sequencing provides\nhighly accurate sequence data, several sequence artifacts, including\nbase calling errors and small insertions/deletions, poor quality reads\nand primer/adapter contamination are quite common in the high throughput\nsequencing data. The primary errors are substitution errors. The error\nrates can vary from 0.5-2.0% with errors mainly rising in frequency at\nthe 3\u2019 ends of reads.  One way to investigate sequence data quality is to visualize the quality\nscores and other metrics in a compact manner to get an idea about the\nquality of a read data set. Read data sets can be improved by pre\nprocessing in different ways like trimming off low quality bases,\ncleaning up any sequencing adapters, removing PCR duplicates and\nscreening for contamination. We can also look at other statistics such\nas, sequence length distribution, base composition, sequence complexity,\npresence of ambiguous bases etc. to assess the overall quality of the\ndata set.  Highly redundant coverage (>15X) of the genome can be used to correct\nsequencing errors in the reads before assembly. Various k-mer based\nerror correction methods exist but are beyond the scope of this\ntutorial.",
            "title": "Introduction"
        },
        {
            "location": "/modules/metagenomics-module-qc/ngs-qc/#quality-value-encoding-schema",
            "text": "In order to use a single character to encode Phred qualities, ASCII\ncharacters are used\n( http://shop.alterlinks.com/ascii-table/ascii-table-us.php ). All ASCII\ncharacters have a decimal number associated with them but the first 32\ncharacters are non-printable (e.g. backspace, shift, return, escape).\nTherefore, the first printable ASCII character is number 33, the\nexclamation mark (!). In Phred+33 encoded quality values the exclamation\nmark takes the Phred quality score of zero.  Early Solexa (now Illumina) sequencing needed to encode negative quality\nvalues. Because ASCII characters $<$ 33 are non-printable, using the\nPhred+33 encoding was not possible. Therefore, they simply moved the\noffset from 33 to 64 thus inventing the Phred+64 encoded quality values.\nIn this encoding a Phred quality of zero is denoted by the ASCII number\n64 (the @ character). Since Illumina 1.8, quality values are now encoded\nusing Phred+33.  FASTQ does not provide a way to describe what quality encoding is used\nfor the quality values. Therefore, you should find this out from your\nsequencing provider. Alternatively, you may be able to figure this out\nby determining what ASCII characters are present in the FASTQ file. E.g\nthe presence of numbers in the quality strings, can only mean the\nquality values are Phred+33 encoded. However, due to the overlapping\nnature of the Phred+33 and Phred+64 encoding schema it is not always\npossible to identify what encoding is in use. For example, if the only\ncharacters seen in the quality string are ( @ABCDEFGHI ), then it is\nimpossible to know if you have really good Phred+33 encoded qualities or\nreally bad Phred+64 encoded qualities.  For a graphical representation of the different ASCII characters used in\nthe two encoding schema see: http://en.wikipedia.org/wiki/FASTQ_format#Encoding",
            "title": "Quality Value Encoding Schema"
        },
        {
            "location": "/modules/metagenomics-module-qc/ngs-qc/#prepare-the-environment",
            "text": "To investigate sequence data quality we will demonstrate tools called\nFastQC and Skewer. FastQC will process and present the reports in a\nvisual manner. Based on the results, the sequence data can be processed\nusing the Skewer. We will use one data set in this practical, which can\nbe found in the QC directory on your desktop.  Open the Terminal and go to the directory where the data are stored:  1\n2\n3 ls\ncd qc\npwd   At any time, help can be displayed for FastQC using the following\ncommand:  1 fastqc -h   Look at SYNOPSIS (Usage) and options after typing fastqc -h",
            "title": "Prepare the Environment"
        },
        {
            "location": "/modules/metagenomics-module-qc/ngs-qc/#quality-visualisation",
            "text": "We have a file for a good quality and bad quality statistics. FastQC\ngenerates results in the form of a zipped and unzipped directory for\neach input file.  Execute the following command on the two files:  1\n2 fastqc -f qcdemo_R1.fastq.gz\nfastqc -f qcdemo_R2.fastq.gz   View the FastQC report file of the bad data using a web browser such as\nfirefox. The \u2019&\u2019 sign puts the job in the background.  1 firefox qcdemo_R2_fastqc.html &   The report file will have a Basic Statistics table and various graphs\nand tables for different quality statistics. E.g.:     Filename  qcdemo_R2.fastq.gz      File type  Conventional base calls    Encoding  Sanger / Illumina 1.9    Total Sequences  1000000    Filtered Sequences  0    Sequence length  150    %GC  37     FastQC Basic Statistics table   Per base sequence quality plot for qcdemo_R2.fastq.gz .   A Phred quality score (or Q-score) expresses an error probability. In\nparticular, it serves as a convenient and compact way to communicate\nvery small error probabilities. The probability that base  A  is wrong  P(sim A)  is expressed by a quality score,  Q(A) , according to the\nrelationship:  Q(A) =-10 log10(P(sim A))  The relationship between the quality score and error probability is\ndemonstrated with the following table:     Quality score  Error probability  Accuracy of the base call      10  0.1  90%    20  0.01  99%    30  0.001  99.9%    40  0.0001  99.99%    50  0.00001  99.999%     Error probabilities associated with various quality (Q) values  [tab:quality_error_probs]    How many sequences were there in your file? What is the read length?   This is a spoiler: {%s%}Hello World.{%ends%}  1 > 1,000,000. read length=150bp     Does the quality score values vary throughout the read length? (hint:\nlook at the \u2019per base sequence quality plot\u2019)   Yes. Quality scores are dropping towards the end of the reads.     What is the quality score range you see?   2-40     At around which position do the scores start falling below Q20 for the 25% quartile range (25%of reads below Q20)?   Around 30 bp position     How can we trim the reads to filter out the low quality data?   By trimming off the bases after a fixed position of the read or by trimming off bases based on the quality score.",
            "title": "Quality Visualisation"
        },
        {
            "location": "/modules/metagenomics-module-qc/ngs-qc/#good-quality-data",
            "text": "View the FastQC report files  fastqc_report.html  to see examples of a\ngood quality data and compare the quality plot with that of the bad_example_fastqc .  1 firefox qcdemo_R1_fastqc.html &   Sequencing errors can complicate the downstream analysis, which normally\nrequires that reads be aligned to each other (for genome assembly) or to\na reference genome (for detection of mutations). Sequence reads\ncontaining errors may lead to ambiguous paths in the assembly or\nimproper gaps. In variant analysis projects sequence reads are aligned\nagainst the reference genome. The errors in the reads may lead to more\nmismatches than expected from mutations alone. But if these errors can\nbe removed or corrected, the read alignments and hence the variant\ndetection will improve. The assemblies will also improve after\npre-processing the reads to remove errors.",
            "title": "Good Quality Data"
        },
        {
            "location": "/modules/metagenomics-module-qc/ngs-qc/#read-trimming",
            "text": "Read trimming can be done in a variety of different ways. Choose a\nmethod which best suits your data. Here we are giving examples of\nfixed-length trimming and quality-based trimming.",
            "title": "Read Trimming"
        },
        {
            "location": "/modules/metagenomics-module-qc/ngs-qc/#quality-based-trimming",
            "text": "Base call quality scores can be used to dynamically determine the trim\npoints for each read. A quality score threshold and minimum read length\nfollowing trimming can be used to remove low quality data.  The previous FastQC results show R1 is fine but R2 has low quality at\nthe end. There is no adaptor contamination though. We will be using\nSkewer to perform the quality trimming.  Run the following command to quality trim a set of paired end data.  1\n2 cd  ~/qc\nskewer -t  20  -l  50   -q  30  -Q  25  -m pe qcdemo_R1.fastq.gz qcdemo_R2.fastq.gz   1\n2\n3\n4\n5\n6\n7\n8\n9 -t :   number of threads to use\n\n-l :   min length to keep after trimming\n\n-q :   Quality threshold used for trimming at 3\u2019 end\n\n-Q :   mean quality threshold for a read\n\n-m :   pair-end mode   Run FastQC on the quality trimmed file and visualise the quality scores.  1\n2\n3\n4 fastqc -f fastq qcdemo_R1.fastq-trimmed-pair1.fastq\nfastqc -f fastq qcdemo_R1.fastq-trimmed-pair2.fastq\nfirefox qcdemo_R1.fastq-trimmed-pair1_fastqc.html &\nfirefox qcdemo_R1.fastq-trimmed-pair2_fastqc.html&   Let\u2019s look at the quality from the second reads. The output should look\nlike:  FastQC Basic Statistics table     Filename  qcdemo_R1.fastq-trimmed-pair2.fastq      File type  Conventional base calls    Encoding  Sanger / Illumina 1.9    Total Sequences  742262    Filtered Sequences  0    Sequence length  50    %GC  37     Per base sequence quality plot for the quality-trimmed  qcdemo_R2.fastq.gz     Did the number of total reads in R1 and R2 change after trimming?    Quality trimming discarded >1000 reads. However, We retain a lot of maximal length reads which have good quality all the way to the ends.    What reads lengths were obtained after quality based trimming?    50-150 Reads <50 bp, following quality trimming, were discarded.    Did you observe adapter sequences in the data?    No. (Hint: look at the overrepresented sequences.    How can you use -a option with fastqc ? (Hint: try fastqc -h).    Adaptors can be supplied in a file for screening.",
            "title": "Quality Based Trimming"
        },
        {
            "location": "/modules/metagenomics-module-qc/ngs-qc/#adapter-clipping",
            "text": "Sometimes sequence reads may end up getting the leftover of adapters and\nprimers used in the sequencing process. It\u2019s good practice to screen\nyour data for these possible contamination for more sensitive alignment\nand assembly based analysis.  This is particularly important when read lengths can be longer than the\nmolecules being sequenced. For example when sequencing miRNAs.  Various QC tools are available to screen and/or clip these\nadapter/primer sequences from your data. Apart from skewer which will be\nusing today the following two tools are also useful for trimming and\nremoving adapter sequence.  Cutadapt: ( http://code.google.com/p/cutadapt/ )\nTrimmomatic:\n( http://www.usadellab.org/cms/?page=trimmomatic )  Here we are demonstrating  Skewer  to trim a given adapter sequence.  1\n2\n3 cd ~/qc\nfastqc -f fastq  adaptorQC.fastq.gz\nskewer -x TGGAATTCTCGGGTGCCAAGGT -t 20 -l 10 -L 35 -q 30 adaptorQC.fastq.gz    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 -x :   adaptor sequence used\n\n-t :   number of threads to use\n\n-l :   min length to keep after trimming\n\n-L :   Max length to keep after trimming, in this experiment we were\n    expecting only small RNA fragments\n\n-Q :   Quality threshold used for trimming at 3\u2019 end. Use -m option to\n    control the end you want to trim   Run FastQC on the adapter trimmed file and visualise the quality scores.\nFastqc now shows adaptor free results.  1\n2 fastqc -f fastq adaptorQC.fastq-trimmed.fastq\nfirefox adaptorQC.fastq-trimmed_fastqc.html &   An alternative tool, not installed on this system, for adapter clipping\nis  fastq-mcf . A list of adapters is provided in a text file. For more\ninformation, see FastqMcf at http://code.google.com/p/ea-utils/wiki/FastqMcf .",
            "title": "Adapter Clipping"
        },
        {
            "location": "/modules/metagenomics-module-qc/ngs-qc/#fixed-length-trimming",
            "text": "We will not cover Fixed Length Trimming but provide the following for your information.  Low quality read ends can be trimmed using a\nfixed-length trimming. We will use the  fastx_trimmer  from the\nFASTX-Toolkit. Usage message to find out various options you can use\nwith this tool. Type  fastx_trimmer -h  at anytime to display help.  We will now do fixed-length trimming of the  bad_example.fastq  file\nusing the following command. You should still be in the qc directory, if\nnot cd back in.  1\n2\n3\n4 cd ~/qc\nfastqc -f fastq bad_example.fastq\nfastx_trimmer -h\nfastx_trimmer -Q 33 -f 1 -l 80 -i bad_example.fastq -o bad_example_trimmed01.fastq   We used the following options in the command above: 1\n2\n3\n4\n5\n6\n7\n8\n9 -Q 33 :   Indicates the input quality scores are Phred+33 encoded\n\n-f :   First base to be retained in the output\n\n-l :   Last base to be retained in the output\n\n-i :   Input FASTQ file name\n\n-o :   Output file name   Run FastQC on the trimmed file and visualise the quality scores of the\ntrimmed file.  1\n2 fastqc -f fastq bad_example_trimmed01.fastq\nfirefox bad_example_trimmed01_fastqc.html &   The output should look like:     Filename  bad_example_trimmed01.fastq      File type  Conventional base call    Encoding  Sanger / Illumina 1.9    Total Sequences  40000    Filtered Sequences  0    Sequence length  80    %GC  48     : FastQC Basic Statistics table   tab:badexampletrimmed   ![Per base sequence quality plot for the fixed-length trimmed bad_example.fastq   What values would you use for  -f  if you wanted to trim off 10 bases at\nthe 5\u2019 end of the reads?  -f 11",
            "title": "Fixed Length Trimming"
        },
        {
            "location": "/modules/metagenomics-module-qc/ngs-qc/#removing-duplicates",
            "text": "Duplicate reads are the ones having the same start and end coordinates.\nThis may be the result of technical duplication (too many PCR cycles),\nor over-sequencing (very high fold coverage). It is very important to\nput the duplication level in context of your experiment. For example,\nduplication level in targeted or re-sequencing projects may mean\nsomething different in RNA-seq experiments. In RNA-seq experiments\noversequencing is usually necessary when detecting low abundance\ntranscripts.  The duplication level computed by FastQC is based on sequence identity\nat the end of reads. Another tool, Picard, determines duplicates based\non identical start and end positions in SAM/BAM alignment files.  We will not cover Picard but provide the following for your\ninformation.  Picard is a suite of tools for performing many common tasks with SAM/BAM\nformat files. For more information see the Picard website and\ninformation about the various command-line tools available:  http://picard.sourceforge.net/command-line-overview.shtml  A good list of tools for filtering PCR duplication can also be found at http://omictools.com/duplicate-reads-removal-c495-p1.html  Picard is installed on this system in  /usr/share/java  One of the Picard tools (MarkDuplicates) can be used to analyse and\nremove duplicates from the raw sequence data. The input for Picard is a\nsorted alignment file in BAM format. Short read aligners such as,\nbowtie, BWA and tophat can be used to align FASTQ files against a\nreference genome to generate SAM/BAM alignment format.  Interested users can use the following general command to run the\nMarkDuplicates tool at their leisure. You only need to provide a BAM\nfile for the INPUT argument (not provided):  1\n2 cd ~/qc\njava -jar /usr/share/java/MarkDuplicates.jar INPUT=<alignment_file.bam> VALIDATION_STRINGENCY=LENIENT OUTPUT=alignment_file.dup METRICS_FILE=alignment_file.matric ASSUME_SORTED=true REMOVE_DUPLICATES=true",
            "title": "Removing Duplicates"
        },
        {
            "location": "/modules/metagenomics-module-tax/tax/",
            "text": "An introduction to taxonomic analysis of amplicon and shotgun data using QIIME\n\u00b6\n\n\nKey Learning Outcomes\n\u00b6\n\n\n\n\nAfter completing this practical the trainee should be able to:\n\n\n\n\n\n\nUnderstand the open source software package QIIME for analysis\n\n\n\n\n\n\nPerform a taxonomic analysis on a 16S rRNA amplicon dataset\n\n\n\n\n\n\nConduct 16S taxonomic analysis on shotgun data\n\n\n\n\n\n\nResources you will be using\n\u00b6\n\n\n\n\nTools Used: Part 1. 16S Analysis\n\u00b6\n\n\nQIIME :  \nhttp://qiime.org/\n\n\nPEAR: \nhttp://sco.h-its.org/exelixis/web/software/pear/doc.html\n\n\nFASTX toolkit (v0.0.14): \nhttp://hannonlab.cshl.edu/fastx_toolkit/download.html\n\n\nBBMap: \nhttp://sourceforge.net/projects/bbmap\n\n\nVSEARCH: \nhttps://github.com/torognes/vsearch\n\n\nSortMeRNA: \nhttp://bioinfo.lifl.fr/RNA/sortmerna/\n\n\nSTAMP: \nhttp://kiwi.cs.dal.ca/Software/STAMP\n\n\nTools Used: Part 2. 16S analysis of whole genome shotgun sequencing\n\u00b6\n\n\nrRNASelector :   \nhttp://www.ezbiocloud.net/sw/rrnaselector\n\n\nMEGAN6 : \nhttp://ab.inf.uni-tuebingen.de/software/megan6\n\n\nUseful Links\n\u00b6\n\n\n\n\nFASTQ Encoding:   \n\n\nhttp://en.wikipedia.org/wiki/FASTQ_format#Encoding\n\n\nQIIME Tutorials:\n\nhttp://qiime.org/tutorials/tutorial.html\n\n\n16S Tutorials:\n\nhttps://github.com/mlangill/microbiome_helper/wiki/16S-tutorial-(chemerin\n)\n\n\nLee et al. (2011). rRNASelector: a computer program for selecting ribosomal RNA encoding sequences from metagenomic and metatranscriptomic shotgun libraries. J. Microbiol. 49(4):689-691.\n\n\nSources of Data\n\u00b6\n\n\n\n\nPart 1: 16S Analysis\n\u00b6\n\n\n\n\n\n\nData : Mouse gut microbial composition affected by the protein chemerin.  \nhttps://www.dropbox.com/s/4fqgi6t3so69224/Sinal_Langille_raw_data.tar.gz\n\n\nhttps://www.dropbox.com/s/r2jqqc7brxg4jhx/16S_chemerin_tutorial.zip\n\n\n\n\n\n\nRDP_trainset16_022016.fa (20 MB) is a subset of the Ribosome Database Project (RDP) filtered to include only bacteria.\n\nhttps://www.dropbox.com/s/qnlsv7ve2lg6qfp/RDP_trainset16_022016.fa?dl=1\n \n\n\n\n\n\n\nPart 2: 16S analysis of whole genome shotgun sequencing\n\u00b6\n\n\n\n\nLi et al. (2013). Draft Genome Sequence of Thermoanaerobacter sp. Strain A7A, Reconstructed from a Metagenome Obtained from a High-Temperature Hydrocarbon Reservoir in the Bass Strait, Australia. Genome Announc. 1(5): e00701-13.\n\n\n\n\nOverview\n\u00b6\n\n\n\n\nIn this tutorial we will look at the open source software package QIIME (pronounced \u2019chime\u2019). QIIME stands for Quantitative Insights Into Microbial Ecology. The package contains many tools that enable users to analyse and compare microbial communities. \n\n\nAfter completion of this tutorial, you should be able to perform a taxonomic analysis on a Illumina pair end 16S rRNA amplicon dataset. In addition you should be able to do 16S rRNA taxonomic analysis on shotgun data using the tool rRNASelector in combination with QIIME and other third party tools.\n\n\nPart 1: 16S Analysis\n\u00b6\n\n\nDe novo OTU picking and diversity analysis using Illumina data\n\u00b6\n\n\n\n\nIntroduction\n\u00b6\n\n\nThe workflow for 16S analysis in general is as follows:\n\n\n\n\nSplit multiplexed reads to samples\n\n\nJoin overlapping read pairs\n\n\nFilter reads on quality and length\n\n\nFilter Chimera sequences\n\n\nAssign reads to samples\n\n\nPick operational taxonomic units (OTUs) for each sample\n\n\nAlpha diversity analysis and rarefaction\n\n\nBeta diversity analysis and Taxonomic composition\n\n\nPCA analysis\n\n\n\n\n16S analysis is a method of microbiome analysis (compared to shotgun metagenomics) that targets the 16S ribosomal RNA gene, as this gene is present in all prokaryotes. It features regions that are conserved among these organisms, as well as variable regions that allow distinction among organisms. These characteristics make this gene useful for analyzing microbial communities at reduced cost compared to metagenomic techniques. A similar workflow can be applied to eukaryotic micro-organisms using the 18S rRNA gene.\n\n\nThe tutorial dataset was originally used in a project to determine whether knocking out the protein chemerin affects gut microbial composition. Originally 116 mouse samples acquired from two different facilities were used for this project (only 24 samples were used in this tutorial dataset, for simplicity). \n\n\nThe Mapping file\n\u00b6\n\n\nMetadata associated with each sample is indicated in the mapping file (map.txt). The mapping file associates the read data files for a sample to it\u2019s metadata. The mapping file can contain information on your experimental design. The format is very strict; columns are separated with a single TAB character; the header names have to be typed exactly as specified in the documentation. A good sample description is useful as it is used in the legends of the figures QIIME generates.\n\n\nIn the mapping (map.txt) file the genotypes of interest can be seen: wildtype (WT), chemerin knockout (chemerin_KO), chemerin receptor knockout (CMKLR1_KO) and a heterozygote for the receptor knockout (HET). Also of importance are the two source facilities: \u201cBZ\u201d and \u201cCJS\u201d. It is important to include as much metadata as possible, so that it can be easily explored later on.\n\n\nOpen Terminal and go to the dataset\u2019s directory:\n\n\n1\n2\ncd\n ~/Desktop/16S_chemerin_tutorial\nls -lhtr\n\n\n\n\n\n\n\u201cfastq\u201d is the directory containing all the sequencing files, which we are going to process. The file \u201cmap.txt\u201d contains metadata about the samples. We can look at it with the less command (hit \u201cq\u201d to exit):\n\n\n1\nless -S map.txt\n\n\n\n\n\n\nThe first column is the sample IDs, the next 2 are blank (note the file is tab-delimited, meaning each column is separated by a tab, not just whitespace) and the 4\nth\n column contains FASTA filenames (these filenames are based on what we will produce in this pipeline). The rest of the columns are metadata about the samples.\n\n\nHere is what the first 4 lines should look like:\n\n\n1\n2\n3\n4\nSampleID       BarcodeSequence LinkerPrimerSequence    FileInput       Source  Mouse#  Cage#   genotype        SamplingWeek\n105CHE6WT                       105CHE6WT_S325_L001.assembled_filtered.nonchimera.fasta BZ      BZ25    7       WT      wk6\n106CHE6WT                       106CHE6WT_S336_L001.assembled_filtered.nonchimera.fasta BZ      BZ26    7       WT      wk6\n107CHE6KO                       107CHE6KO_S347_L001.assembled_filtered.nonchimera.fasta BZ      BZ27    7       chemerin_KO     wk6\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe Barcode and LinkerPrimerSequence are absent for this tutorial, these would be used for assigning multiplexed reads to samples and for quality control. Our tutorial data set is already de-multiplexed. What QIIME could be used? \n\n\nExecute the following command and test the mapping file for potential errors:\n\n\n1\nvalidate_mapping_file.py -m map.txt -o map_output\n\n\n\n\n\n\nThere shouldn\u2019t be any errors, but if errors occur a corrected mapping file will be written to the directory map_output\n\n\nJoin directory of PE reads\n\u00b6\n\n\nFirst we need to join the overlapping Illumina Pair End (PE) reads contained in the fastq directory for each sample.\n\n\n1\n2\ncd ~/Desktop/16S_chemerin_tutorial  \nrun_pear.pl -p 4 -o stitched_reads fastq/*fastq\n\n\n\n\n\n\n1\n2\n\"-p 4\" indicates this job should be run on 4 CPU \n\"-o stitched_reads\" indicates that the output folder)\n\n\n\n\n\n\nFour FASTQ files will be generated for each set of paired-end reads:\n    (1) assembled reads (used for downstream analyses)\n    (2) discarded reads (often abnormal reads, e.g. large run of Ns).\n    (3) unassembled forward reads\n    (4) unassembled reverse reads\n\n\nThe default log file \u201cpear_summary_log.txt\u201d contains the percent of reads either assembled, discarded or unassembled.\n\n\n\n\nQuestion 1\n\n\nWhat percent of reads were successfully stitched for sample 40CMK6WT?\n\n\n\n\nFiltering reads by quality and length\n\u00b6\n\n\nWe will now filter our reads on a quality score cut-off of 30 over 90% of bases and at a maximum length of 400 bp, which are considered reasonable filtering criteria (~2 min on 1 CPU):\n\n\n1\n2\ncd\n ~/Desktop/16S_chemerin_tutorial\nread_filter.pl -q \n30\n -p \n90\n -l \n400\n -thread \n4\n -c both stitched_reads/*.assembled*fastq\n\n\n\n\n\n\nThe \n\u201c-c both\u201d\n option above checks for the default forward and reverse degenerate primer sequences to match exactly in each sequences (You can use whatever primer sequences you want. However, the primer sequences in this case are set by default in the script, which you can look at with \n\u201cread_filter.pl -h\u201d\n). If you don\u2019t set the \n\u201c-c\u201d\n option to either \n\u201cboth\u201d\n or \n\u201cforward\u201d\n then there wont be any primer matching.\n\n\nBy default this script will output filtered FASTQs in a folder called \n\u201cfiltered_reads\u201d\n and the percent of reads thrown out after each filtering step is recorded in \n\u201cread_filter_log.txt\u201d\n. This script is just a convenient way to run two popular tools for read filtering: \nFASTX-toolkit and BBMAP\n.\n\n\nIf you look in this logfile you will note that ~40% of reads were filtered out for each sample. You can also see the counts and percent of reads dropped at each step.\n\n\n\n\nQuestion 2\n\n\nHow many reads of sample 36CMK6WT were filtered out for not containing a match to the forward primer (which is the default setting in this case).\n\n\n\n\nConversion to FASTA and removal of chimeric reads\n\u00b6\n\n\nThe next steps in the pipeline require the simple conversion of sequences from FASTQ to FASTA format, using the following command (< 1 min on 1 CPU):\n\n\n1\n2\ncd\n ~/Desktop/16S_chemerin_tutorial\nrun_fastq_to_fasta.pl -p \n4\n -o fasta_files filtered_reads/*fastq\n\n\n\n\n\n\nNote that this command removes any sequences containing \u201cN\u201d (a fully ambiguous base read), which is << 1% of the reads after the read filtering steps above.\n\n\nDuring PCA amplification 16S rRNA sequences from different organisms can sometimes combine to form hybrid molecules called chimeric sequences. It\u2019s important to remove these so they aren\u2019t incorrectly called as novel Operational Taxonomic Units (OTUs). Unfortunately, not all chimeric reads will be removed during this step, which is something to keep in mind during the next steps.\n\n\nYou can run chimera checking with VSEARCH with this command (~3 min on 1 CPU):\n\n\n1\nchimera_filter.pl -type \n1\n -thread \n4\n -db /home/shared/rRNA_db/Bacteria_RDP_trainset15_092015.fa fasta_files/*fasta\n\n\n\n\n\n\nThis script will remove any reads called either ambiguously or as chimeric, and output the remaining reads in the \u201cnon_chimeras\u201d folder by default.\n\n\nBy default the logfile \u201cchimeraFilter_log.txt\u201d is generated containing the counts and percentages of reads filtered out for each sample.\n\n\n\n\nQuestion 3\n\n\nWhat is the mean percent of reads retained after this step, based on the output in the log file (\u201cnonChimeraCallsPercent\u201d column)?\n\n\n\n\n\n\nQuestion 4\n\n\nWhat percent of stitched reads was retained for sample 75CMK8KO after all the filtering steps \n\n\n\n\n\n\nHint\n\n\n\u201cyou will need to compare the original number of reads to the number of reads output by chimera_filter.pl)?\u201d\n\n\n\n\nAssign samples to the reads\n\u00b6\n\n\nNow that we have adequately prepared the reads, we can now run OTU picking using QIIME. An Operational Taxonomic Unit (\nOTU\n) defines a taxonomic group based on sequence similarity among sampled organisms. QIIME software clusters sequence reads from microbial communities in order to classify its constituent micro-organisms into OTUs. QIIME requires FASTA files to be input in a specific format (specifically, sample names need to be at the beginning of each header line). We have provided the mapping file (\n\u201cmap.txt\u201d\n), which links filenames to sample names and metadata.\n\n\nAs we saw the map.txt (e.g. with \nless -S\n) had 2 columns without any data: \u201c\nBarcodeSequence\n\u201d and \u201c\nLinkerPrimerSequence\n\u201d. We don\u2019t need to use these columns today. These would normally be populated for multiplexed data.\n\n\nAlso, you will see that the \u201c\nFileInput\n\u201d column contains the names of each FASTA file, which is what we need to specify for the command below.\n\n\nThis command will correctly format the input FASTA files and output a single FASTA:\n\n\n1\nadd_qiime_labels.py -i non_chimeras/ -m map.txt -c FileInput -o combined_fasta \n\n\n\n\n\n\nIf you take a look at \ncombined_fasta/combined_seqs.fna\n you can see that the first column of header line is a sample name taken from the mapping file.\n\n\nDe novo OTU picking\n\u00b6\n\n\nNow that the input file has been correctly formatted we can run the actual OTU picking program.\n\n\nSeveral parameters for this program can be specified into a text file, which will be read in by \u201cpick_open_reference_otus.py\u201d:\n\n\n1\n2\n3\necho\n \n\"pick_otus:threads 1\"\n >> clustering_params.txt\n\necho\n \n\"pick_otus:sortmerna_coverage 0.8\"\n >> clustering_params.txt\n\necho\n \n\"pick_otus:sortmerna_db /home/shared/pick_otu_indexdb_rna/97_otus\"\n >> clustering_params.txt\n\n\n\n\n\n\nWe will be using the \nuclust method\n of \nopen-reference OTU picking\n. In open-reference OTU picking, reads are first clustered against a reference database; then, a certain percent (10% in the below command) of those reads that failed to be classified are sub-sampled to create a new reference database and the remaining unclassified reads are clustered against this new database. This \nde novo\n clustering step is repeated again by default using the below command (can be turned off to save time with the \n\u201c\u2013suppress_step4\u201d\n option).\n\n\nWe are actually also retaining singletons (i.e. OTUs identified by 1 read), which we will then remove in the next step. Note that \u201c\n$PWD\n\u201d is just a variable that contains the path to your current directory. This command takes ~7 min with 1 CPU. Lowering the \u201c\n-s\n\u201d parameter\u2019s value will greatly affect running speed.\n\n\n1\npick_open_reference_otus.py -i \n$PWD\n/combined_fasta/combined_seqs.fna -o \n$PWD\n/clustering/ -p \n$PWD\n/clustering_params.txt -m sortmerna_sumaclust -s \n0\n.1 -v --min_otu_size \n1\n\n\n\n\n\n\n\nRemove low confidence OTUs\n\u00b6\n\n\nWe will now remove low confidence OTUs, i.e. those that are called by a low number of reads. It\u2019s difficult to choose a hard cut-off for how many reads are needed for an OTU to be confidently called, since of course OTUs are often at low frequency within a community. A reasonable approach is to remove any OTU identified by fewer than 0.1% of the reads, given that 0.1% is the estimated amount of sample bleed-through between runs on the Illumina Miseq:\n\n\n1\nremove_low_confidence_otus.py -i $PWD/clustering/otu_table_mc1_w_tax_no_pynast_failures.biom -o $PWD/clustering/otu_table_high_conf.biom\n\n\n\n\n\n\nSince we are just doing a test run with few sequences, the threshold is 1 OTU regardless. However, this is an important step with real datasets. Note: Sequence errors can give rise to spurious OTUs, we can filter out OTUs that only contain a single sequence (singletons). QIIME allows you to do this quite easily, or you could also remove abundant taxa if you are more interested in rare taxa.\n\n\nWe can compare the summaries of these two BIOM files:\n\n\n1\n2\n3\nbiom summarize-table -i clustering/otu_table_mc1_w_tax_no_pynast_failures.biom -o clustering/otu_table_mc1_w_tax_no_pynast_failures_summary.txt\n\nbiom summarize-table -i clustering/otu_table_high_conf.biom -o clustering/otu_table_high_conf_summary.txt\n\n\n\n\n\n\nThe first four lines of clustering/otu_table_mc1_w_tax_no_pynast_failures_summary.txt are:\n\n\n1\n2\n3\n4\n5\nNum samples: 24\nNum observations: 2420\nTotal count: 12014\nTable density (fraction of non-zero values): 0.097\nThis means that for the 24 separate samples, 2420 OTUs were called based on 12014 reads. Only 9.7% of the values in the sample x OTU table are non-zero, meaning that most OTUs are in a small number of samples.\n\n\n\n\n\n\nIn contrast, the first four lines of clustering/otu_table_high_conf_summary.txt are:\n\n\n1\n2\n3\n4\nNum samples: 24\nNum observations: 884\nTotal count: 10478\nTable density (fraction of non-zero values): 0.193\n\n\n\n\n\n\nAfter removing low-confidence OTUs, only \n36.5%\n were retained: \nthe number of OTUs dropped from 2420 to 884\n. This effect is generally even more drastic for bigger datasets. However, the numbers of reads only dropped from 12014 to 10478 (so \n87% of the reads were retained\n). You can also see that the table density increased, as we would expect.\n\n\nThe pipeline creates a Newick-formatted phylogenetic\ntree **(\n.tre)*\n in the clustering directory.\nYou can run the program \u2019figtree\u2019 from the terminal, a graphic interface will be launched by typing \u2019figtree\u2019 then hit the return key. \n\n\n1\nfigtree\n\n\n\n\nView the tree by opening the file \u2019*.tre\u2019 in the \u2019clustering\u2019 folder \n(Desktop->Taxonomy->otus)\n. The tree that is produced is too complex to be of much use. We will look at a different tool, Megan 6, which produces a far more useful tree. \n\n\nMegan can be opened from the terminal by typing \nMEGAN\n. If you are asked for a licence select the following file /mnt/workshop/data/HT_MEGAN5_registration_for_academic_use.txt. From the File menu select Import -> BIOM format.\nFind your biom file and import it.\n\n\nMegan will generate a tree that is far more informative than the one produced with FigTree. You can change the way Megan displays the data by clicking on the various icons and menu items. Please spend some time exploring your data.\n\n\nThe Word Cloud visualization is interesting, too, if you want to find out which samples are similar and which samples stand out.\n\n\nView OTU statistics\n\u00b6\n\n\nYou can generate some statistics, e.g. the number of reads assigned, distribution among samples. Some of the statistics are useful for further downstream analysis, e.g. beta-diversity analysis.  \nWrite down the minimum value under Counts/sample summary\n. We need it for beta-diversity analysis.\nYou can look at the read depth per sample in \nclustering/otu_table_high_conf_summary.txt\n, here are the first five samples (they are sorted from smallest to largest):\n\n\n1\n2\n3\n4\n5\n6\nCounts/sample detail:\n106CHE6WT: 375.0\n111CHE6KO: 398.0\n39CMK6KO: 410.0\n113CHE6WT: 412.0\n108CHE6KO: 413.0\n\n\n\n\n\n\n\n\nQuestion 5\n\n\nWhat is the read depth for sample \u201c75CMK8KO\u201d?\n\n\n\n\nWe need to subsample the number of reads for each sample to the same depth, which is necessary for several downstream analyses. This is called \nrarefaction\n, a technique that provides an indication of \nspecies richness\n for a given number of samples. First it indicates if you have sequence enough to identify all species. Second we want to rarify the read depth of samples to a similar number of reads for comparative analysis. There is actually quite a lot of debate about whether rarefaction is necessary (since it throws out data), but it is still the standard method used in microbiome studies. We want to rarify the read depth to the sample with the lowest \u201creasonable\u201d number of reads. Of course, a \u201creasonable\u201d read depth is quite subjective and depends on how much variation there is between samples.\n\n\nRarify reads\n\u00b6\n\n\n1\n2\nmkdir final_otu_tables\nsingle_rarefaction.py -i clustering/otu_table_high_conf.biom -o final_otu_tables/otu_table.biom -d \n355\n\n\n\n\n\n\n\nVisualize taxonomic composition\n\u00b6\n\n\n\n\nWe will now group sequences by taxonomic assignment at various levels. The following command produces a number of charts that can be viewed in a browser. The command takes about 5 minutes to complete\n\n\n1\nsummarize_taxa_through_plots.py -i final_otu_tables/otu_table.biom -o  wf_taxa_summary -m map.txt \n\n\n\n\n\n\nTo view the output, open a web browser from the Applications ->Internet menu. You can use Google chrome, Firefox or Chromium. In  Firefox use the File menu to select \n\n\n1\n2\nDesktop ->;Taxonomy ->; wf_taxa_summary ->; taxa_summary_plots \nand open either area_charts.html or bar_chars.html. \n\n\n\n\n\n\nI prefer the bar charts myself. The top chart visualizes taxonomic composition at phylum level for each of the samples. The next chart goes down to class level and following charts go another level up again. The charts (particularly the ones more at the top) are very useful for discovering how the communities in your samples differ from each other. \n\n\nAlpha diversity within samples and rarefaction curves\n\u00b6\n\n\n\n\nAlpha diversity is the microbial diversity within a sample. QIIME can calculate a lot of metrics, but for our tutorial, we generate 3 metrics from the alpha rarefaction workflow: chao1 (estimates species richness); observed species metric (the count of unique OTUs); phylogenetic distance. The following workflow generates rarefaction plots to visualize alpha diversity.\n\n\nRun the following command from within your taxonomy directory, this should take a few minutes:\n\n\n1\nalpha_rarefaction.py -i final_otu_tables/otu_table.biom -o plots/alpha_rarefaction_plot -t clustering/rep_set.tre --min_rare_depth \n40\n --max_rare_depth \n355\n -m map.txt  --num_steps \n10\n\n\n\n\n\n\n\nFirst we are going to view the rarefaction curves in a web browser by opening the resulting HTML file to view the plots: \n\n\n1\nplots/alpha_rarefaction_plot/alpha_rarefaction_plots/rarefaction_plots.html\n\n\n\n\n\n\nChoose \u201cobserved_otus\u201d as the metric and \u201cSource\u201d as the category. You should see this plot:\n\n\n\n\nThere is no difference in the number of OTUs identified in the guts of mice from the BZ facility than the CJS facility, based on this dataset. However, since the rarefaction curves have not reached a plateau, it is likely that this comparison is just incorrect to make with so few reads. Indeed, with the full dataset you do see a difference in the number of OTUs.\n\n\nIn general the more reads you have, the more OTUs you will observe. If a rarefaction curve start to flatten, it means that you have probably sequenced at sufficient depth, in other words, producing more reads will not significantly add more OTUs. If on the other hand hasn\u2019t flattened, you have not sampled enough to capture enough of the microbial diversity and by extrapolating the curve you may be able to estimate how many more reads you will need. Consult the QIIME overview tutorial for further information.\n\n\nRun the following command from within your taxonomy directory, this should take a few minutes to generate a heatmap of the level three taxonomy:\n\n\n1\nmake_otu_heatmap.py -i final_otu_tables/otu_table_L3.biom -o final_otu_tables/otu_table_L3_heatmap.pdf -c Treatment -m map.txt\n\n\n\n\n\n\nBeta diversity and beta diversity plots\n\u00b6\n\n\nBeta diversity analysis, is the assessment of differences between microbial communities/samples. As we have already observed, our samples contain different numbers of sequences. The first step is to remove sample heterogeneity by randomly selecting the same number of reads from every sample. This number corresponds to the \u2019minimum\u2019 number recorded when you looked at the OTU statistics.  Now run the following command\n\n\n1\nbeta_diversity_through_plots.py -i final_otu_tables/otu_table.biom -m map.txt -o bdiv_even -t otus/rep_set.tre -e \n355\n\n\n\n\n\n\n\nGood data quality and sample metadata is important for visualising metagenomics analysis. The output of these comparisons is a square matrix where a distance or dissimilarity is calculated between every pair of community samples, reflecting the dissimilarity between those samples. The data distance matrix can be then visualized with analyses such as PCoA and hierarchical clustering.\n\n\nTesting for statistical differences\n\u00b6\n\n\nSo what\u2019s the next step? Since we know that source facility is such an important factor, we could analyze samples from each facility separately. This will lower our statistical power to detect a signal, but otherwise we cannot easily test for a difference between genotypes.\n\n\nTo compare the genotypes within the two source facilities separately we fortunately don\u2019t need to re-run the OTU-picking. Instead, we can just take different subsets of samples from the final OTU table. First though we need to make two new mapping files with the samples we want to subset:\n\n\n1\n2\nhead -n \n1\n map.txt >> map_BZ.txt\n;\n awk \n'{ if ( $3 == \"BZ\" ) { print $0 } }'\n map.txt >>map_BZ.txt\nhead -n \n1\n map.txt >> map_CJS.txt\n;\n awk \n'{ if ( $3 == \"CJS\" ) { print $0 } }'\n map.txt >>map_CJS.txt\n\n\n\n\n\n\nThese commands are split into 2 parts (separated by \u201c;\u201d). The first part writes the header line to each new mapping file. The second part is an awk command that prints any line where the 3\nrd\n column equals the id of the source facility. Note that awk splits by any whitespace by default, which is why the source facility IDs are in the 3\nrd\n column according to awk, even though we know this isn\u2019t true when the file is tab-delimited.\n\n\nThe BIOM \u201csubset-table\u201d command requires a text file with 1 sample name per line, which we can generate by these quick bash commands:\n\n\n1\n2\ntail -n +2  map_BZ.txt \n|\n awk \n'{print $1}'\n > samples_BZ.txt\ntail -n +2  map_CJS.txt \n|\n awk \n'{print $1}'\n > samples_CJS.txt\n\n\n\n\n\n\nThese commands mean that the first line (the header) should be ignored and then the first column should be printed to a new file. We can now take the two subsets of samples from the BIOM file:\n\n\n1\n2\nbiom subset-table -i final_otu_tables/otu_table.biom -a sample -s samples_BZ.txt -o final_otu_tables/otu_table_BZ.biom\nbiom subset-table -i final_otu_tables/otu_table.biom -a sample -s samples_CJS.txt -o final_otu_tables/otu_table_CJS.biom\n\n\n\n\nWe can now re-create the beta diversity plots for each subset:\n\n\n1\n2\nbeta_diversity_through_plots.py -m map_BZ.txt -t clustering/rep_set.tre -i final_otu_tables/otu_table_BZ.biom -o plots/bdiv_otu_BZ \nbeta_diversity_through_plots.py -m map_CJS.txt -t clustering/rep_set.tre -i final_otu_tables/otu_table_CJS.biom -o plots/bdiv_otu_CJS\n\n\n\n\nWe can now take a look at whether the genotypes separate in the re-generated weighted beta diversity PCoAs for each source facility separately.\n\n\nFor the BZ source facility:\n\n\n\n\nAnd for the CJS source facility:\n\n\n\n\nJust by looking at these PCoA plots it\u2019s clear that if there is any difference it\u2019s subtle. To statistically evaluate whether the weighted UniFrac beta diversities differ among genotypes within each source facility, you can run an analysis of similarity (ANOSIM) test. These commands will run the ANOSIM test and change the output filename:\n\n\n1\n2\ncompare_categories.py --method anosim -i plots/bdiv_otu_BZ/weighted_unifrac_dm.txt -m map_BZ.txt -c genotype -o beta_div_tests\nmv beta_div_tests/anosim_results.txt  beta_div_tests/anosim_results_BZ.txt \n\n\n\n\n\n1\n2\ncompare_categories.py --method anosim -i plots/bdiv_otu_CJS/weighted_unifrac_dm.txt -m map_CJS.txt -c genotype -o beta_div_tests\nmv beta_div_tests/anosim_results.txt  beta_div_tests/anosim_results_CJS.txt\n\n\n\n\n\nYou can take a look at the output files to see significance values and test statistics. The P-values for both tests are > 0.05, so there is no significant difference in the UniFrac beta diversities of different genotypes within each source facility.\n\n\nUniFrac beta diversity analysis\n\u00b6\n\n\nUniFrac is a particular beta-diversity measure that analyzes dissimilarity between samples, sites, or communities. We will now create UniFrac beta diversity (both \nweighted\n and \nunweighted\n) principal coordinates analysis (\nPCoA\n) plots. PCoA plots are related to principal components analysis (PCA) plots, but are based on any dissimilarity matrix rather than just a covariance/correlation matrix. \n\n\nNote\n the major difference between weighted and unweighted analysis is the inclusion of OTU abundance when calculating distances between communities. You should use weighted if the biological question you are trying to ask takes OTU abundance of your groups into consideration. If some samples are forming groups with weighted, then it\u2019s likely the larger or smaller abundances of several OTUs are the primary driving force in PCoA space, but when all OTUs are considered at equal abundance, these differences are lost (unweighted).\n\n\nQIIME \u201c\nbeta_diversity_through_plots.py\n\u201d takes the OTU table as input, as well as file which contains the phylogenetic relatedness between all clustered OTUs. One HTML file will be generated for the weighted and unweighted beta diversity distances:\n\n\n1\n2\nplots/bdiv_otu/weighted_unifrac_emperor_pcoa_plot/index.html\nplots/bdiv_otu/unweighted_unifrac_emperor_pcoa_plot/index.html\n\n\n\n\n\n\nOpen the weighted HTML file in your browser and take a look, you should see a PCoA very similar to this:\n\n\n\n\nThe actual metadata we are most interested in for this dataset is the \u201cgenotype\u201d column of the mapping file, which contains the different genotypes I described at the beginning of this tutorial. Go to the \u201cColors\u201d tab of the Emperor plot (which is what we were just looking at) and change the selection from \u201cBarcodeSequence\u201d (default) to \u201cgenotype\u201d. You should see a similar plot to this:\n\n\n\n\nThe WT genotype is spread out across both knockout genotypes, which is not what we would have predicted.\n\n\nYou\u2019ll see what\u2019s really driving the differences in beta diversity when you change the selection under the \u201cColors\u201d tab from \u201cgenotype\u201d to \u201cSource\u201d:\n\n\n\n\nUsing STAMP to test for particular differences\n\u00b6\n\n\nOften we\u2019re interested in figuring out which particular taxa (or other feature such as functions) differs in relative abundance between groups. There are many ways this can be done, but one common method is to use the easy-to-use program \nSTAMP\n. We\u2019ll run STAMP on the full OTU table to figure out which genera differ between the two source facilities as an example.\n\n\nBefore running STAMP we need to convert our OTU table into a format that STAMP can read:\n\n\n1\nbiom_to_stamp.py -m taxonomy final_otu_tables/otu_table.biom >final_otu_tables/otu_table.spf\n\n\n\n\n\n\nIf you take a look at \u201cfinal_otu_tables/otu_table.spf\u201d with less you\u2019ll see that it\u2019s just a simple tab-delimited table.\n\n\nNow we\u2019re ready to open up STAMP, which you can either do by typing STAMP on the command-line or by clicking the side-bar icon.\n\n\nLoad \u201cotu_table.spf\u201d as the Profile file and \u201cmap.txt\u201d as the Group metadata file.\n\n\nAs a reminder, the full paths of these files should be:\n\n1\n/home/mh_user/Desktop/16S_chemerin_tutorial/final_otu_tables/otu_table.spf and /home/mh_user/Desktop/16S_chemerin_tutorial/map.txt\n\n\n\n\n\nChange the Group field to \u201cSource\u201d and the profile level to \u201cLevel_6\u201d (which corresponds to the genus level). Change the heading from \u201cMultiple groups\u201d to \u201cTwo groups\u201d. The statistical test to \u201cWelch\u2019s t-test\u201d and the multiple test correction to \u201cBenjamini-Hochberg FDR\u201d\n\n\nChange the plot type to \u201cBar plot\u201d. Look at the barplot for Prevotella and save it to a file.\n\n\n\n\nQuestion\n\n\nCan you see how many genera are significant by clicking \u201cShow only active features\u201d?\n\n\n\n\nPart 2: 16S analysis of whole genome shotgun sequencing\n\u00b6\n\n\nClosed reference OTU picking of 16S ribosomal rRNA fragments selected from a shotgun data set\n\u00b6\n\n\n\n\nIn a closed-reference OTU picking process, reads are clustered against a reference sequence collection and any reads, which do not hit a sequence in the reference sequence collection, are excluded from downstream analyses. In QIIME, \npick_closed_reference_otus.py\n is the primary interface for closed-reference OTU picking in QIIME. If the user provides taxonomic assignments for sequences in the reference database,\nthose are assigned to OTUs. We could use this approach to perform\ntaxonomic analysis on shotgun data. We need to perform the following\nsteps:\n\n\n\n\nExtract those reads from the data set that contain 16S ribosomal RNA\nsequence. If there are less than (e.g.) 100 nucleotides of rRNA\nsequence, the read should be discarded.\n\n\nRemove non-rRNA sequence (flanking regions) from those reads\n\n\nRun closed-reference OTU picking workflow\n\n\nVisualise the results, e.g. in Megan\n\n\n\n\nExtraction of 16S rRNA sequence-containing reads with rRNASelector\n\u00b6\n\n\n\n\nWe will analyze an Illumina paired-end dataset that has been drastically reduced in size for this tutorial, while preserving the majority of the 16S containing reads. The dataset is from the metagenome described at\n\nhttp://www.ncbi.nlm.nih.gov/pmc/articles/PMC3772140/\n. There is a pdf in the working directory for this part of the tutorial. This is a paired end dataset, and where read pairs overlapped, they were merged into a single sequence. If read pairs did not overlap, both reads were included in the analysis. QC was performed using the EBI Metagenomics pipeline. We will use a tool called rRNASelector, which is freely available\n(\nhttp://www.ncbi.nlm.nih.gov/pubmed/21887657\n) to select our 16S rRNA sequence containing reads. The tool invokes hmmsearch and uses trained hidden Markov models to detect reads with 16S rRNA sequence. The tool also trims the reads so that only 16S rRNA sequence is present in the fasta file we will feed into the QIIME workflow.\n\n\nFirst, we need to go to our working directory. You will find a file called A7A-paired.fasta containing the sequence reads. Fire up \nrRNASelector\n from the command line.\n\n\n1\n2\ncd ~/Desktop/Taxonomy/A7A/\nrRNASelector\n\n\n\n\n\n\nA graphical interface should appear. Note interaction with the interface may have a few seconds lag. \n\n\n\n\nLoad the sequence file by clicking on \u2019File Choose\u2019 at the top and navigate to the file A7A-paired.fasta. \n\n\nSelect the file and click \u2019Open\u2019. The tool will automatically fill in file names for the result files.\n\n\nChange the Number of CPUs to \u20194\u2019 \n\n\nSelect Prokaryote 16S (to include both bacterial and archaeal 16S sequences) \n\n\nSpecify the location of the hmmsearch file by clicking the second \u2019File Choose\u2019 button. Type in manually the location \u2018\n/usr/bin/hmmsearch\n\u2019, \n\n\nThen click process. The run should take a few minutes to complete.\n\n\n\n\nIf all went well, you can close rRNASelector by clicking on Exit. You will have 3 new files in your directory, one containing untrimmed 16S reads, one containing trimmed 16S reads (\nA7A-paired.prok.16s.trim.fasta\n; that\u2019s the one we want) and a file containing reads that do not contain (sufficient) 16S sequence.\n\n\nClosed-reference OTU picking workflow and visualization of results in Megan 6\n\u00b6\n\n\n\n\nWe are now ready to pick our OTUs. We do that by running the following\ncommand (all on one line and no space after gg_otus-12-10):\n\n\n1\npick_closed_reference_otus.py -i A7A-paired.prok.16s.trim.fasta -o ./cr_uc -r /mnt/workshop/tools/qiime_software/gg_otus-12_10-release/rep_set/97_otus.fasta -t /mnt/workshop/tools/qiime_software/gg_otus-12_10-release/taxonomy/97_otu_taxonomy.txt\n\n\n\n\n\n\nWe need to specify the following options. The command will take several minutes to run. When finished open Megan as described before, import the otu_table.biom file and explore the results.\n\n\n1\n2\n3\n4\n-i input_file.fasta\n-o output_directory\n-r /path/to/reference_sequences\n-t /path/to/reference_taxonomy\n\n\n\n\n\n\nBonus\n\u00b6\n\n\n\n\nThe QIIME overview tutorial at\n(\nhttp://qiime.org/tutorials/tutorial.html\n) has a number of additional steps that you may find interesting; so feel free to try some of them out. Note hat we have not installed Cytoscape, so we cannot visualize OTU networks.\n\n\nWe will end this tutorial with a summary of what we have done and how well our analysis compares with the one in the paper.\n\n\nHopefully you will have acquired new skills that allow you to tackle your own taxonomic analyses. There are many more tutorials on the QIIME website that can help you pick the best strategy for your project (\nhttp://qiime.org/tutorials/\n) and \nhttps://github.com/mlangill/microbiome_helper/wiki/\n. There are alternatives that might suit your need better (e.g. VAMPS at \nhttp://vamps.mbl.ed\n>; mothur at \nhttp://www.mothur.org\n) and others.",
            "title": "Metagenomics Taxonomic"
        },
        {
            "location": "/modules/metagenomics-module-tax/tax/#an-introduction-to-taxonomic-analysis-of-amplicon-and-shotgun-data-using-qiime",
            "text": "",
            "title": "An introduction to taxonomic analysis of amplicon and shotgun data using QIIME"
        },
        {
            "location": "/modules/metagenomics-module-tax/tax/#key-learning-outcomes",
            "text": "After completing this practical the trainee should be able to:    Understand the open source software package QIIME for analysis    Perform a taxonomic analysis on a 16S rRNA amplicon dataset    Conduct 16S taxonomic analysis on shotgun data",
            "title": "Key Learning Outcomes"
        },
        {
            "location": "/modules/metagenomics-module-tax/tax/#resources-you-will-be-using",
            "text": "",
            "title": "Resources you will be using"
        },
        {
            "location": "/modules/metagenomics-module-tax/tax/#tools-used-part-1-16s-analysis",
            "text": "QIIME :   http://qiime.org/  PEAR:  http://sco.h-its.org/exelixis/web/software/pear/doc.html  FASTX toolkit (v0.0.14):  http://hannonlab.cshl.edu/fastx_toolkit/download.html  BBMap:  http://sourceforge.net/projects/bbmap  VSEARCH:  https://github.com/torognes/vsearch  SortMeRNA:  http://bioinfo.lifl.fr/RNA/sortmerna/  STAMP:  http://kiwi.cs.dal.ca/Software/STAMP",
            "title": "Tools Used: Part 1. 16S Analysis"
        },
        {
            "location": "/modules/metagenomics-module-tax/tax/#tools-used-part-2-16s-analysis-of-whole-genome-shotgun-sequencing",
            "text": "rRNASelector :    http://www.ezbiocloud.net/sw/rrnaselector  MEGAN6 :  http://ab.inf.uni-tuebingen.de/software/megan6",
            "title": "Tools Used: Part 2. 16S analysis of whole genome shotgun sequencing"
        },
        {
            "location": "/modules/metagenomics-module-tax/tax/#useful-links",
            "text": "FASTQ Encoding:     http://en.wikipedia.org/wiki/FASTQ_format#Encoding  QIIME Tutorials: http://qiime.org/tutorials/tutorial.html  16S Tutorials: https://github.com/mlangill/microbiome_helper/wiki/16S-tutorial-(chemerin )  Lee et al. (2011). rRNASelector: a computer program for selecting ribosomal RNA encoding sequences from metagenomic and metatranscriptomic shotgun libraries. J. Microbiol. 49(4):689-691.",
            "title": "Useful Links"
        },
        {
            "location": "/modules/metagenomics-module-tax/tax/#sources-of-data",
            "text": "",
            "title": "Sources of Data"
        },
        {
            "location": "/modules/metagenomics-module-tax/tax/#part-1-16s-analysis",
            "text": "Data : Mouse gut microbial composition affected by the protein chemerin.   https://www.dropbox.com/s/4fqgi6t3so69224/Sinal_Langille_raw_data.tar.gz  https://www.dropbox.com/s/r2jqqc7brxg4jhx/16S_chemerin_tutorial.zip    RDP_trainset16_022016.fa (20 MB) is a subset of the Ribosome Database Project (RDP) filtered to include only bacteria. https://www.dropbox.com/s/qnlsv7ve2lg6qfp/RDP_trainset16_022016.fa?dl=1",
            "title": "Part 1: 16S Analysis"
        },
        {
            "location": "/modules/metagenomics-module-tax/tax/#part-2-16s-analysis-of-whole-genome-shotgun-sequencing",
            "text": "Li et al. (2013). Draft Genome Sequence of Thermoanaerobacter sp. Strain A7A, Reconstructed from a Metagenome Obtained from a High-Temperature Hydrocarbon Reservoir in the Bass Strait, Australia. Genome Announc. 1(5): e00701-13.",
            "title": "Part 2: 16S analysis of whole genome shotgun sequencing"
        },
        {
            "location": "/modules/metagenomics-module-tax/tax/#overview",
            "text": "In this tutorial we will look at the open source software package QIIME (pronounced \u2019chime\u2019). QIIME stands for Quantitative Insights Into Microbial Ecology. The package contains many tools that enable users to analyse and compare microbial communities.   After completion of this tutorial, you should be able to perform a taxonomic analysis on a Illumina pair end 16S rRNA amplicon dataset. In addition you should be able to do 16S rRNA taxonomic analysis on shotgun data using the tool rRNASelector in combination with QIIME and other third party tools.",
            "title": "Overview"
        },
        {
            "location": "/modules/metagenomics-module-tax/tax/#part-1-16s-analysis_1",
            "text": "",
            "title": "Part 1: 16S Analysis"
        },
        {
            "location": "/modules/metagenomics-module-tax/tax/#de-novo-otu-picking-and-diversity-analysis-using-illumina-data",
            "text": "",
            "title": "De novo OTU picking and diversity analysis using Illumina data"
        },
        {
            "location": "/modules/metagenomics-module-tax/tax/#introduction",
            "text": "The workflow for 16S analysis in general is as follows:   Split multiplexed reads to samples  Join overlapping read pairs  Filter reads on quality and length  Filter Chimera sequences  Assign reads to samples  Pick operational taxonomic units (OTUs) for each sample  Alpha diversity analysis and rarefaction  Beta diversity analysis and Taxonomic composition  PCA analysis   16S analysis is a method of microbiome analysis (compared to shotgun metagenomics) that targets the 16S ribosomal RNA gene, as this gene is present in all prokaryotes. It features regions that are conserved among these organisms, as well as variable regions that allow distinction among organisms. These characteristics make this gene useful for analyzing microbial communities at reduced cost compared to metagenomic techniques. A similar workflow can be applied to eukaryotic micro-organisms using the 18S rRNA gene.  The tutorial dataset was originally used in a project to determine whether knocking out the protein chemerin affects gut microbial composition. Originally 116 mouse samples acquired from two different facilities were used for this project (only 24 samples were used in this tutorial dataset, for simplicity).",
            "title": "Introduction"
        },
        {
            "location": "/modules/metagenomics-module-tax/tax/#the-mapping-file",
            "text": "Metadata associated with each sample is indicated in the mapping file (map.txt). The mapping file associates the read data files for a sample to it\u2019s metadata. The mapping file can contain information on your experimental design. The format is very strict; columns are separated with a single TAB character; the header names have to be typed exactly as specified in the documentation. A good sample description is useful as it is used in the legends of the figures QIIME generates.  In the mapping (map.txt) file the genotypes of interest can be seen: wildtype (WT), chemerin knockout (chemerin_KO), chemerin receptor knockout (CMKLR1_KO) and a heterozygote for the receptor knockout (HET). Also of importance are the two source facilities: \u201cBZ\u201d and \u201cCJS\u201d. It is important to include as much metadata as possible, so that it can be easily explored later on.  Open Terminal and go to the dataset\u2019s directory:  1\n2 cd  ~/Desktop/16S_chemerin_tutorial\nls -lhtr   \u201cfastq\u201d is the directory containing all the sequencing files, which we are going to process. The file \u201cmap.txt\u201d contains metadata about the samples. We can look at it with the less command (hit \u201cq\u201d to exit):  1 less -S map.txt   The first column is the sample IDs, the next 2 are blank (note the file is tab-delimited, meaning each column is separated by a tab, not just whitespace) and the 4 th  column contains FASTA filenames (these filenames are based on what we will produce in this pipeline). The rest of the columns are metadata about the samples.  Here is what the first 4 lines should look like:  1\n2\n3\n4 SampleID       BarcodeSequence LinkerPrimerSequence    FileInput       Source  Mouse#  Cage#   genotype        SamplingWeek\n105CHE6WT                       105CHE6WT_S325_L001.assembled_filtered.nonchimera.fasta BZ      BZ25    7       WT      wk6\n106CHE6WT                       106CHE6WT_S336_L001.assembled_filtered.nonchimera.fasta BZ      BZ26    7       WT      wk6\n107CHE6KO                       107CHE6KO_S347_L001.assembled_filtered.nonchimera.fasta BZ      BZ27    7       chemerin_KO     wk6    Note   The Barcode and LinkerPrimerSequence are absent for this tutorial, these would be used for assigning multiplexed reads to samples and for quality control. Our tutorial data set is already de-multiplexed. What QIIME could be used?   Execute the following command and test the mapping file for potential errors:  1 validate_mapping_file.py -m map.txt -o map_output   There shouldn\u2019t be any errors, but if errors occur a corrected mapping file will be written to the directory map_output",
            "title": "The Mapping file"
        },
        {
            "location": "/modules/metagenomics-module-tax/tax/#join-directory-of-pe-reads",
            "text": "First we need to join the overlapping Illumina Pair End (PE) reads contained in the fastq directory for each sample.  1\n2 cd ~/Desktop/16S_chemerin_tutorial  \nrun_pear.pl -p 4 -o stitched_reads fastq/*fastq   1\n2 \"-p 4\" indicates this job should be run on 4 CPU \n\"-o stitched_reads\" indicates that the output folder)   Four FASTQ files will be generated for each set of paired-end reads:\n    (1) assembled reads (used for downstream analyses)\n    (2) discarded reads (often abnormal reads, e.g. large run of Ns).\n    (3) unassembled forward reads\n    (4) unassembled reverse reads  The default log file \u201cpear_summary_log.txt\u201d contains the percent of reads either assembled, discarded or unassembled.   Question 1  What percent of reads were successfully stitched for sample 40CMK6WT?",
            "title": "Join directory of PE reads"
        },
        {
            "location": "/modules/metagenomics-module-tax/tax/#filtering-reads-by-quality-and-length",
            "text": "We will now filter our reads on a quality score cut-off of 30 over 90% of bases and at a maximum length of 400 bp, which are considered reasonable filtering criteria (~2 min on 1 CPU):  1\n2 cd  ~/Desktop/16S_chemerin_tutorial\nread_filter.pl -q  30  -p  90  -l  400  -thread  4  -c both stitched_reads/*.assembled*fastq   The  \u201c-c both\u201d  option above checks for the default forward and reverse degenerate primer sequences to match exactly in each sequences (You can use whatever primer sequences you want. However, the primer sequences in this case are set by default in the script, which you can look at with  \u201cread_filter.pl -h\u201d ). If you don\u2019t set the  \u201c-c\u201d  option to either  \u201cboth\u201d  or  \u201cforward\u201d  then there wont be any primer matching.  By default this script will output filtered FASTQs in a folder called  \u201cfiltered_reads\u201d  and the percent of reads thrown out after each filtering step is recorded in  \u201cread_filter_log.txt\u201d . This script is just a convenient way to run two popular tools for read filtering:  FASTX-toolkit and BBMAP .  If you look in this logfile you will note that ~40% of reads were filtered out for each sample. You can also see the counts and percent of reads dropped at each step.   Question 2  How many reads of sample 36CMK6WT were filtered out for not containing a match to the forward primer (which is the default setting in this case).",
            "title": "Filtering reads by quality and length"
        },
        {
            "location": "/modules/metagenomics-module-tax/tax/#conversion-to-fasta-and-removal-of-chimeric-reads",
            "text": "The next steps in the pipeline require the simple conversion of sequences from FASTQ to FASTA format, using the following command (< 1 min on 1 CPU):  1\n2 cd  ~/Desktop/16S_chemerin_tutorial\nrun_fastq_to_fasta.pl -p  4  -o fasta_files filtered_reads/*fastq   Note that this command removes any sequences containing \u201cN\u201d (a fully ambiguous base read), which is << 1% of the reads after the read filtering steps above.  During PCA amplification 16S rRNA sequences from different organisms can sometimes combine to form hybrid molecules called chimeric sequences. It\u2019s important to remove these so they aren\u2019t incorrectly called as novel Operational Taxonomic Units (OTUs). Unfortunately, not all chimeric reads will be removed during this step, which is something to keep in mind during the next steps.  You can run chimera checking with VSEARCH with this command (~3 min on 1 CPU):  1 chimera_filter.pl -type  1  -thread  4  -db /home/shared/rRNA_db/Bacteria_RDP_trainset15_092015.fa fasta_files/*fasta   This script will remove any reads called either ambiguously or as chimeric, and output the remaining reads in the \u201cnon_chimeras\u201d folder by default.  By default the logfile \u201cchimeraFilter_log.txt\u201d is generated containing the counts and percentages of reads filtered out for each sample.   Question 3  What is the mean percent of reads retained after this step, based on the output in the log file (\u201cnonChimeraCallsPercent\u201d column)?    Question 4  What percent of stitched reads was retained for sample 75CMK8KO after all the filtering steps     Hint  \u201cyou will need to compare the original number of reads to the number of reads output by chimera_filter.pl)?\u201d",
            "title": "Conversion to FASTA and removal of chimeric reads"
        },
        {
            "location": "/modules/metagenomics-module-tax/tax/#assign-samples-to-the-reads",
            "text": "Now that we have adequately prepared the reads, we can now run OTU picking using QIIME. An Operational Taxonomic Unit ( OTU ) defines a taxonomic group based on sequence similarity among sampled organisms. QIIME software clusters sequence reads from microbial communities in order to classify its constituent micro-organisms into OTUs. QIIME requires FASTA files to be input in a specific format (specifically, sample names need to be at the beginning of each header line). We have provided the mapping file ( \u201cmap.txt\u201d ), which links filenames to sample names and metadata.  As we saw the map.txt (e.g. with  less -S ) had 2 columns without any data: \u201c BarcodeSequence \u201d and \u201c LinkerPrimerSequence \u201d. We don\u2019t need to use these columns today. These would normally be populated for multiplexed data.  Also, you will see that the \u201c FileInput \u201d column contains the names of each FASTA file, which is what we need to specify for the command below.  This command will correctly format the input FASTA files and output a single FASTA:  1 add_qiime_labels.py -i non_chimeras/ -m map.txt -c FileInput -o combined_fasta    If you take a look at  combined_fasta/combined_seqs.fna  you can see that the first column of header line is a sample name taken from the mapping file.",
            "title": "Assign samples to the reads"
        },
        {
            "location": "/modules/metagenomics-module-tax/tax/#de-novo-otu-picking",
            "text": "Now that the input file has been correctly formatted we can run the actual OTU picking program.  Several parameters for this program can be specified into a text file, which will be read in by \u201cpick_open_reference_otus.py\u201d:  1\n2\n3 echo   \"pick_otus:threads 1\"  >> clustering_params.txt echo   \"pick_otus:sortmerna_coverage 0.8\"  >> clustering_params.txt echo   \"pick_otus:sortmerna_db /home/shared/pick_otu_indexdb_rna/97_otus\"  >> clustering_params.txt   We will be using the  uclust method  of  open-reference OTU picking . In open-reference OTU picking, reads are first clustered against a reference database; then, a certain percent (10% in the below command) of those reads that failed to be classified are sub-sampled to create a new reference database and the remaining unclassified reads are clustered against this new database. This  de novo  clustering step is repeated again by default using the below command (can be turned off to save time with the  \u201c\u2013suppress_step4\u201d  option).  We are actually also retaining singletons (i.e. OTUs identified by 1 read), which we will then remove in the next step. Note that \u201c $PWD \u201d is just a variable that contains the path to your current directory. This command takes ~7 min with 1 CPU. Lowering the \u201c -s \u201d parameter\u2019s value will greatly affect running speed.  1 pick_open_reference_otus.py -i  $PWD /combined_fasta/combined_seqs.fna -o  $PWD /clustering/ -p  $PWD /clustering_params.txt -m sortmerna_sumaclust -s  0 .1 -v --min_otu_size  1",
            "title": "De novo OTU picking"
        },
        {
            "location": "/modules/metagenomics-module-tax/tax/#remove-low-confidence-otus",
            "text": "We will now remove low confidence OTUs, i.e. those that are called by a low number of reads. It\u2019s difficult to choose a hard cut-off for how many reads are needed for an OTU to be confidently called, since of course OTUs are often at low frequency within a community. A reasonable approach is to remove any OTU identified by fewer than 0.1% of the reads, given that 0.1% is the estimated amount of sample bleed-through between runs on the Illumina Miseq:  1 remove_low_confidence_otus.py -i $PWD/clustering/otu_table_mc1_w_tax_no_pynast_failures.biom -o $PWD/clustering/otu_table_high_conf.biom   Since we are just doing a test run with few sequences, the threshold is 1 OTU regardless. However, this is an important step with real datasets. Note: Sequence errors can give rise to spurious OTUs, we can filter out OTUs that only contain a single sequence (singletons). QIIME allows you to do this quite easily, or you could also remove abundant taxa if you are more interested in rare taxa.  We can compare the summaries of these two BIOM files:  1\n2\n3 biom summarize-table -i clustering/otu_table_mc1_w_tax_no_pynast_failures.biom -o clustering/otu_table_mc1_w_tax_no_pynast_failures_summary.txt\n\nbiom summarize-table -i clustering/otu_table_high_conf.biom -o clustering/otu_table_high_conf_summary.txt   The first four lines of clustering/otu_table_mc1_w_tax_no_pynast_failures_summary.txt are:  1\n2\n3\n4\n5 Num samples: 24\nNum observations: 2420\nTotal count: 12014\nTable density (fraction of non-zero values): 0.097\nThis means that for the 24 separate samples, 2420 OTUs were called based on 12014 reads. Only 9.7% of the values in the sample x OTU table are non-zero, meaning that most OTUs are in a small number of samples.   In contrast, the first four lines of clustering/otu_table_high_conf_summary.txt are:  1\n2\n3\n4 Num samples: 24\nNum observations: 884\nTotal count: 10478\nTable density (fraction of non-zero values): 0.193   After removing low-confidence OTUs, only  36.5%  were retained:  the number of OTUs dropped from 2420 to 884 . This effect is generally even more drastic for bigger datasets. However, the numbers of reads only dropped from 12014 to 10478 (so  87% of the reads were retained ). You can also see that the table density increased, as we would expect.  The pipeline creates a Newick-formatted phylogenetic\ntree **( .tre)*  in the clustering directory.\nYou can run the program \u2019figtree\u2019 from the terminal, a graphic interface will be launched by typing \u2019figtree\u2019 then hit the return key.   1 figtree  \nView the tree by opening the file \u2019*.tre\u2019 in the \u2019clustering\u2019 folder  (Desktop->Taxonomy->otus) . The tree that is produced is too complex to be of much use. We will look at a different tool, Megan 6, which produces a far more useful tree.   Megan can be opened from the terminal by typing  MEGAN . If you are asked for a licence select the following file /mnt/workshop/data/HT_MEGAN5_registration_for_academic_use.txt. From the File menu select Import -> BIOM format.\nFind your biom file and import it.  Megan will generate a tree that is far more informative than the one produced with FigTree. You can change the way Megan displays the data by clicking on the various icons and menu items. Please spend some time exploring your data.  The Word Cloud visualization is interesting, too, if you want to find out which samples are similar and which samples stand out.",
            "title": "Remove low confidence OTUs"
        },
        {
            "location": "/modules/metagenomics-module-tax/tax/#view-otu-statistics",
            "text": "You can generate some statistics, e.g. the number of reads assigned, distribution among samples. Some of the statistics are useful for further downstream analysis, e.g. beta-diversity analysis.   Write down the minimum value under Counts/sample summary . We need it for beta-diversity analysis.\nYou can look at the read depth per sample in  clustering/otu_table_high_conf_summary.txt , here are the first five samples (they are sorted from smallest to largest):  1\n2\n3\n4\n5\n6 Counts/sample detail:\n106CHE6WT: 375.0\n111CHE6KO: 398.0\n39CMK6KO: 410.0\n113CHE6WT: 412.0\n108CHE6KO: 413.0    Question 5  What is the read depth for sample \u201c75CMK8KO\u201d?   We need to subsample the number of reads for each sample to the same depth, which is necessary for several downstream analyses. This is called  rarefaction , a technique that provides an indication of  species richness  for a given number of samples. First it indicates if you have sequence enough to identify all species. Second we want to rarify the read depth of samples to a similar number of reads for comparative analysis. There is actually quite a lot of debate about whether rarefaction is necessary (since it throws out data), but it is still the standard method used in microbiome studies. We want to rarify the read depth to the sample with the lowest \u201creasonable\u201d number of reads. Of course, a \u201creasonable\u201d read depth is quite subjective and depends on how much variation there is between samples.",
            "title": "View OTU statistics"
        },
        {
            "location": "/modules/metagenomics-module-tax/tax/#rarify-reads",
            "text": "1\n2 mkdir final_otu_tables\nsingle_rarefaction.py -i clustering/otu_table_high_conf.biom -o final_otu_tables/otu_table.biom -d  355",
            "title": "Rarify reads"
        },
        {
            "location": "/modules/metagenomics-module-tax/tax/#visualize-taxonomic-composition",
            "text": "We will now group sequences by taxonomic assignment at various levels. The following command produces a number of charts that can be viewed in a browser. The command takes about 5 minutes to complete  1 summarize_taxa_through_plots.py -i final_otu_tables/otu_table.biom -o  wf_taxa_summary -m map.txt    To view the output, open a web browser from the Applications ->Internet menu. You can use Google chrome, Firefox or Chromium. In  Firefox use the File menu to select   1\n2 Desktop ->;Taxonomy ->; wf_taxa_summary ->; taxa_summary_plots \nand open either area_charts.html or bar_chars.html.    I prefer the bar charts myself. The top chart visualizes taxonomic composition at phylum level for each of the samples. The next chart goes down to class level and following charts go another level up again. The charts (particularly the ones more at the top) are very useful for discovering how the communities in your samples differ from each other.",
            "title": "Visualize taxonomic composition"
        },
        {
            "location": "/modules/metagenomics-module-tax/tax/#alpha-diversity-within-samples-and-rarefaction-curves",
            "text": "Alpha diversity is the microbial diversity within a sample. QIIME can calculate a lot of metrics, but for our tutorial, we generate 3 metrics from the alpha rarefaction workflow: chao1 (estimates species richness); observed species metric (the count of unique OTUs); phylogenetic distance. The following workflow generates rarefaction plots to visualize alpha diversity.  Run the following command from within your taxonomy directory, this should take a few minutes:  1 alpha_rarefaction.py -i final_otu_tables/otu_table.biom -o plots/alpha_rarefaction_plot -t clustering/rep_set.tre --min_rare_depth  40  --max_rare_depth  355  -m map.txt  --num_steps  10    First we are going to view the rarefaction curves in a web browser by opening the resulting HTML file to view the plots:   1 plots/alpha_rarefaction_plot/alpha_rarefaction_plots/rarefaction_plots.html   Choose \u201cobserved_otus\u201d as the metric and \u201cSource\u201d as the category. You should see this plot:   There is no difference in the number of OTUs identified in the guts of mice from the BZ facility than the CJS facility, based on this dataset. However, since the rarefaction curves have not reached a plateau, it is likely that this comparison is just incorrect to make with so few reads. Indeed, with the full dataset you do see a difference in the number of OTUs.  In general the more reads you have, the more OTUs you will observe. If a rarefaction curve start to flatten, it means that you have probably sequenced at sufficient depth, in other words, producing more reads will not significantly add more OTUs. If on the other hand hasn\u2019t flattened, you have not sampled enough to capture enough of the microbial diversity and by extrapolating the curve you may be able to estimate how many more reads you will need. Consult the QIIME overview tutorial for further information.  Run the following command from within your taxonomy directory, this should take a few minutes to generate a heatmap of the level three taxonomy:  1 make_otu_heatmap.py -i final_otu_tables/otu_table_L3.biom -o final_otu_tables/otu_table_L3_heatmap.pdf -c Treatment -m map.txt",
            "title": "Alpha diversity within samples and rarefaction curves"
        },
        {
            "location": "/modules/metagenomics-module-tax/tax/#beta-diversity-and-beta-diversity-plots",
            "text": "Beta diversity analysis, is the assessment of differences between microbial communities/samples. As we have already observed, our samples contain different numbers of sequences. The first step is to remove sample heterogeneity by randomly selecting the same number of reads from every sample. This number corresponds to the \u2019minimum\u2019 number recorded when you looked at the OTU statistics.  Now run the following command  1 beta_diversity_through_plots.py -i final_otu_tables/otu_table.biom -m map.txt -o bdiv_even -t otus/rep_set.tre -e  355    Good data quality and sample metadata is important for visualising metagenomics analysis. The output of these comparisons is a square matrix where a distance or dissimilarity is calculated between every pair of community samples, reflecting the dissimilarity between those samples. The data distance matrix can be then visualized with analyses such as PCoA and hierarchical clustering.",
            "title": "Beta diversity and beta diversity plots"
        },
        {
            "location": "/modules/metagenomics-module-tax/tax/#testing-for-statistical-differences",
            "text": "So what\u2019s the next step? Since we know that source facility is such an important factor, we could analyze samples from each facility separately. This will lower our statistical power to detect a signal, but otherwise we cannot easily test for a difference between genotypes.  To compare the genotypes within the two source facilities separately we fortunately don\u2019t need to re-run the OTU-picking. Instead, we can just take different subsets of samples from the final OTU table. First though we need to make two new mapping files with the samples we want to subset:  1\n2 head -n  1  map.txt >> map_BZ.txt ;  awk  '{ if ( $3 == \"BZ\" ) { print $0 } }'  map.txt >>map_BZ.txt\nhead -n  1  map.txt >> map_CJS.txt ;  awk  '{ if ( $3 == \"CJS\" ) { print $0 } }'  map.txt >>map_CJS.txt   These commands are split into 2 parts (separated by \u201c;\u201d). The first part writes the header line to each new mapping file. The second part is an awk command that prints any line where the 3 rd  column equals the id of the source facility. Note that awk splits by any whitespace by default, which is why the source facility IDs are in the 3 rd  column according to awk, even though we know this isn\u2019t true when the file is tab-delimited.  The BIOM \u201csubset-table\u201d command requires a text file with 1 sample name per line, which we can generate by these quick bash commands:  1\n2 tail -n +2  map_BZ.txt  |  awk  '{print $1}'  > samples_BZ.txt\ntail -n +2  map_CJS.txt  |  awk  '{print $1}'  > samples_CJS.txt   These commands mean that the first line (the header) should be ignored and then the first column should be printed to a new file. We can now take the two subsets of samples from the BIOM file:  1\n2 biom subset-table -i final_otu_tables/otu_table.biom -a sample -s samples_BZ.txt -o final_otu_tables/otu_table_BZ.biom\nbiom subset-table -i final_otu_tables/otu_table.biom -a sample -s samples_CJS.txt -o final_otu_tables/otu_table_CJS.biom  \nWe can now re-create the beta diversity plots for each subset:  1\n2 beta_diversity_through_plots.py -m map_BZ.txt -t clustering/rep_set.tre -i final_otu_tables/otu_table_BZ.biom -o plots/bdiv_otu_BZ \nbeta_diversity_through_plots.py -m map_CJS.txt -t clustering/rep_set.tre -i final_otu_tables/otu_table_CJS.biom -o plots/bdiv_otu_CJS  \nWe can now take a look at whether the genotypes separate in the re-generated weighted beta diversity PCoAs for each source facility separately.  For the BZ source facility:   And for the CJS source facility:   Just by looking at these PCoA plots it\u2019s clear that if there is any difference it\u2019s subtle. To statistically evaluate whether the weighted UniFrac beta diversities differ among genotypes within each source facility, you can run an analysis of similarity (ANOSIM) test. These commands will run the ANOSIM test and change the output filename:  1\n2 compare_categories.py --method anosim -i plots/bdiv_otu_BZ/weighted_unifrac_dm.txt -m map_BZ.txt -c genotype -o beta_div_tests\nmv beta_div_tests/anosim_results.txt  beta_div_tests/anosim_results_BZ.txt    1\n2 compare_categories.py --method anosim -i plots/bdiv_otu_CJS/weighted_unifrac_dm.txt -m map_CJS.txt -c genotype -o beta_div_tests\nmv beta_div_tests/anosim_results.txt  beta_div_tests/anosim_results_CJS.txt   You can take a look at the output files to see significance values and test statistics. The P-values for both tests are > 0.05, so there is no significant difference in the UniFrac beta diversities of different genotypes within each source facility.",
            "title": "Testing for statistical differences"
        },
        {
            "location": "/modules/metagenomics-module-tax/tax/#unifrac-beta-diversity-analysis",
            "text": "UniFrac is a particular beta-diversity measure that analyzes dissimilarity between samples, sites, or communities. We will now create UniFrac beta diversity (both  weighted  and  unweighted ) principal coordinates analysis ( PCoA ) plots. PCoA plots are related to principal components analysis (PCA) plots, but are based on any dissimilarity matrix rather than just a covariance/correlation matrix.   Note  the major difference between weighted and unweighted analysis is the inclusion of OTU abundance when calculating distances between communities. You should use weighted if the biological question you are trying to ask takes OTU abundance of your groups into consideration. If some samples are forming groups with weighted, then it\u2019s likely the larger or smaller abundances of several OTUs are the primary driving force in PCoA space, but when all OTUs are considered at equal abundance, these differences are lost (unweighted).  QIIME \u201c beta_diversity_through_plots.py \u201d takes the OTU table as input, as well as file which contains the phylogenetic relatedness between all clustered OTUs. One HTML file will be generated for the weighted and unweighted beta diversity distances:  1\n2 plots/bdiv_otu/weighted_unifrac_emperor_pcoa_plot/index.html\nplots/bdiv_otu/unweighted_unifrac_emperor_pcoa_plot/index.html   Open the weighted HTML file in your browser and take a look, you should see a PCoA very similar to this:   The actual metadata we are most interested in for this dataset is the \u201cgenotype\u201d column of the mapping file, which contains the different genotypes I described at the beginning of this tutorial. Go to the \u201cColors\u201d tab of the Emperor plot (which is what we were just looking at) and change the selection from \u201cBarcodeSequence\u201d (default) to \u201cgenotype\u201d. You should see a similar plot to this:   The WT genotype is spread out across both knockout genotypes, which is not what we would have predicted.  You\u2019ll see what\u2019s really driving the differences in beta diversity when you change the selection under the \u201cColors\u201d tab from \u201cgenotype\u201d to \u201cSource\u201d:",
            "title": "UniFrac beta diversity analysis"
        },
        {
            "location": "/modules/metagenomics-module-tax/tax/#using-stamp-to-test-for-particular-differences",
            "text": "Often we\u2019re interested in figuring out which particular taxa (or other feature such as functions) differs in relative abundance between groups. There are many ways this can be done, but one common method is to use the easy-to-use program  STAMP . We\u2019ll run STAMP on the full OTU table to figure out which genera differ between the two source facilities as an example.  Before running STAMP we need to convert our OTU table into a format that STAMP can read:  1 biom_to_stamp.py -m taxonomy final_otu_tables/otu_table.biom >final_otu_tables/otu_table.spf   If you take a look at \u201cfinal_otu_tables/otu_table.spf\u201d with less you\u2019ll see that it\u2019s just a simple tab-delimited table.  Now we\u2019re ready to open up STAMP, which you can either do by typing STAMP on the command-line or by clicking the side-bar icon.  Load \u201cotu_table.spf\u201d as the Profile file and \u201cmap.txt\u201d as the Group metadata file.  As a reminder, the full paths of these files should be: 1 /home/mh_user/Desktop/16S_chemerin_tutorial/final_otu_tables/otu_table.spf and /home/mh_user/Desktop/16S_chemerin_tutorial/map.txt   Change the Group field to \u201cSource\u201d and the profile level to \u201cLevel_6\u201d (which corresponds to the genus level). Change the heading from \u201cMultiple groups\u201d to \u201cTwo groups\u201d. The statistical test to \u201cWelch\u2019s t-test\u201d and the multiple test correction to \u201cBenjamini-Hochberg FDR\u201d  Change the plot type to \u201cBar plot\u201d. Look at the barplot for Prevotella and save it to a file.   Question  Can you see how many genera are significant by clicking \u201cShow only active features\u201d?",
            "title": "Using STAMP to test for particular differences"
        },
        {
            "location": "/modules/metagenomics-module-tax/tax/#part-2-16s-analysis-of-whole-genome-shotgun-sequencing_1",
            "text": "",
            "title": "Part 2: 16S analysis of whole genome shotgun sequencing"
        },
        {
            "location": "/modules/metagenomics-module-tax/tax/#closed-reference-otu-picking-of-16s-ribosomal-rrna-fragments-selected-from-a-shotgun-data-set",
            "text": "In a closed-reference OTU picking process, reads are clustered against a reference sequence collection and any reads, which do not hit a sequence in the reference sequence collection, are excluded from downstream analyses. In QIIME,  pick_closed_reference_otus.py  is the primary interface for closed-reference OTU picking in QIIME. If the user provides taxonomic assignments for sequences in the reference database,\nthose are assigned to OTUs. We could use this approach to perform\ntaxonomic analysis on shotgun data. We need to perform the following\nsteps:   Extract those reads from the data set that contain 16S ribosomal RNA\nsequence. If there are less than (e.g.) 100 nucleotides of rRNA\nsequence, the read should be discarded.  Remove non-rRNA sequence (flanking regions) from those reads  Run closed-reference OTU picking workflow  Visualise the results, e.g. in Megan",
            "title": "Closed reference OTU picking of 16S ribosomal rRNA fragments selected from a shotgun data set"
        },
        {
            "location": "/modules/metagenomics-module-tax/tax/#extraction-of-16s-rrna-sequence-containing-reads-with-rrnaselector",
            "text": "We will analyze an Illumina paired-end dataset that has been drastically reduced in size for this tutorial, while preserving the majority of the 16S containing reads. The dataset is from the metagenome described at http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3772140/ . There is a pdf in the working directory for this part of the tutorial. This is a paired end dataset, and where read pairs overlapped, they were merged into a single sequence. If read pairs did not overlap, both reads were included in the analysis. QC was performed using the EBI Metagenomics pipeline. We will use a tool called rRNASelector, which is freely available\n( http://www.ncbi.nlm.nih.gov/pubmed/21887657 ) to select our 16S rRNA sequence containing reads. The tool invokes hmmsearch and uses trained hidden Markov models to detect reads with 16S rRNA sequence. The tool also trims the reads so that only 16S rRNA sequence is present in the fasta file we will feed into the QIIME workflow.  First, we need to go to our working directory. You will find a file called A7A-paired.fasta containing the sequence reads. Fire up  rRNASelector  from the command line.  1\n2 cd ~/Desktop/Taxonomy/A7A/\nrRNASelector   A graphical interface should appear. Note interaction with the interface may have a few seconds lag.    Load the sequence file by clicking on \u2019File Choose\u2019 at the top and navigate to the file A7A-paired.fasta.   Select the file and click \u2019Open\u2019. The tool will automatically fill in file names for the result files.  Change the Number of CPUs to \u20194\u2019   Select Prokaryote 16S (to include both bacterial and archaeal 16S sequences)   Specify the location of the hmmsearch file by clicking the second \u2019File Choose\u2019 button. Type in manually the location \u2018 /usr/bin/hmmsearch \u2019,   Then click process. The run should take a few minutes to complete.   If all went well, you can close rRNASelector by clicking on Exit. You will have 3 new files in your directory, one containing untrimmed 16S reads, one containing trimmed 16S reads ( A7A-paired.prok.16s.trim.fasta ; that\u2019s the one we want) and a file containing reads that do not contain (sufficient) 16S sequence.",
            "title": "Extraction of 16S rRNA sequence-containing reads with rRNASelector"
        },
        {
            "location": "/modules/metagenomics-module-tax/tax/#closed-reference-otu-picking-workflow-and-visualization-of-results-in-megan-6",
            "text": "We are now ready to pick our OTUs. We do that by running the following\ncommand (all on one line and no space after gg_otus-12-10):  1 pick_closed_reference_otus.py -i A7A-paired.prok.16s.trim.fasta -o ./cr_uc -r /mnt/workshop/tools/qiime_software/gg_otus-12_10-release/rep_set/97_otus.fasta -t /mnt/workshop/tools/qiime_software/gg_otus-12_10-release/taxonomy/97_otu_taxonomy.txt   We need to specify the following options. The command will take several minutes to run. When finished open Megan as described before, import the otu_table.biom file and explore the results.  1\n2\n3\n4 -i input_file.fasta\n-o output_directory\n-r /path/to/reference_sequences\n-t /path/to/reference_taxonomy",
            "title": "Closed-reference OTU picking workflow and visualization of results in Megan 6"
        },
        {
            "location": "/modules/metagenomics-module-tax/tax/#bonus",
            "text": "The QIIME overview tutorial at\n( http://qiime.org/tutorials/tutorial.html ) has a number of additional steps that you may find interesting; so feel free to try some of them out. Note hat we have not installed Cytoscape, so we cannot visualize OTU networks.  We will end this tutorial with a summary of what we have done and how well our analysis compares with the one in the paper.  Hopefully you will have acquired new skills that allow you to tackle your own taxonomic analyses. There are many more tutorials on the QIIME website that can help you pick the best strategy for your project ( http://qiime.org/tutorials/ ) and  https://github.com/mlangill/microbiome_helper/wiki/ . There are alternatives that might suit your need better (e.g. VAMPS at  http://vamps.mbl.ed >; mothur at  http://www.mothur.org ) and others.",
            "title": "Bonus"
        },
        {
            "location": "/modules/metagenomics-module-vis/vis/",
            "text": "Analysis Visualisation\n\u00b6\n\n\nKey Learning Outcomes\n\u00b6\n\n\n\n\nAfter completing this module the trainee should be able to:\n\n\n\n\n\n\nVisualise between sample comparisons, using Emperor PCA\n\n\n\n\n\n\nUnderstand the difference between weighted and unweighted analysis\n\n\n\n\n\n\nResources You\u2019ll be Using\n\u00b6\n\n\n\n\nTools Used\n\u00b6\n\n\nEmperor: \n\n\nhttp://qiime.org/emperor/tutorial_index.html\n\n\nData sets\n\u00b6\n\n\n\n\nSutton et al. (2013). Impact of Long-Term Diesel Contamination on Soil Microbial Community Structure: \n\n\nhttp://www.ncbi.nlm.nih.gov/pmc/articles/PMC3553749/pdf/zam619.pdf\n\n\nFierer et al. (2010). Forensic identification using skin bacterial communities: \n\n\nhttp://www.ncbi.nlm.nih.gov/pmc/articles/PMC2852011/\n\n\nIntroduction\n\u00b6\n\n\n\n\nGood data quality and sample metadata is important for visualising\nmetagenomics analysis.\n\n\nFor this tutorial we are using Emperor a browser enabled scatter plot\nvisualisation tool. We will be using the Sutton and Fierer data sets for\nviewing the principal components analysis (PCoA) from the QIIME Beta\ndiversity analysis (16S).\n\n\nBeta Diversity computation and plots\n\u00b6\n\n\n\n\nBeta diversity represents between sample comparisons based on their\ncomposition. The output of these comparisons is a square matrix where a\n\u201cdistance\u201d or dissimilarity is calculated between every pair of\ncommunity samples, reflecting the dissimilarity between those samples.\nThe data distance matrix can be then visualized with analyses such as\nPCoA and hierarchical clustering. PCoA is a technique that maps the\nsamples in the distance matrix to a set of axes that show the maximum\namount of variation explained. The principal coordinates can be plotted\nin two or three dimensions to provide an intuitive visualization of the\ndata structure to look at differences between the samples, and look for\nsimilarities by sample conditions.\n\n\nThe Beta diversity workflow you ran earlier contained a number of steps:\n\n\n\n\n\n\nRarifying the OTU table to remove sample heterogeneity. A rarified\n    OTU table should be used so that artificial diversity induced due to\n    different sampling effort is removed.\n\n\n\n\n\n\nCalculating the Beta diversity, the unifrac weighted and unweighted\n    generated principal coorodinates are created here by default.\n\n\n\n\n\n\nThe following are the required options and inputs for Beta diversity\nanalysis:\n\n\n1\n2\n3\n4\n5\n1.  Option -i, OTU table (*.biom)\n2.  Option \u2013t, phylogeny tree (*.tre) from pick_de_novo_otus.py\n3.  Option \u2013m, the user-defined mapping file\n4.  Option \u2013o, the output directory\n5.  Option \u2013e, the number of sequences per sample (sequencing depth)\n\n\n\n\n\n\nYou do not need to run the following command, it may overwrite the\npre-computed analysis. This is the command you ran on the Sutton sub\ndataset.\n\n\n1\nbeta_diversity_through_plots.py -i otus/otu_table.biom -m mapping.txt -o wf_bdiv_even122/ -t otus/rep_set.tre -e 122\n\n\n\n\n\n\nPrepare the environment\n\u00b6\n\n\nThe data for this practical can be found in the Taxonomy directory on\nyour desktop. Go to the pre-computed Beta diversity analysis in the\nTaxonomy directory.\n\n\nOpen the Terminal and go to where the data is stored. Investigate the\ndirectories available.\n\n\n1\n2\ncd ~/Desktop/Taxonomy/sutton_full_denoised/\nls \u2013lhtr\n\n\n\n\n\n\nOpen the following files: wf_bdiv_even394/unweighted_unifrac_pc.txt\nand wf_bdiv_even394/weighted_unifrac_pc.txt using the \u2018less\u2019\ncommand. The option \u2013S with \u2018less\u2019 allows you to view files with\nunwrapped lines, to escape type q (for quit). To scroll left and right,\nuse the arrow keys.\n\n\n1\n2\nless \u2013S wf_bdiv_even394/unweighted_unifrac_pc.txt\nless \u2013S wf_bdiv_even394/weighted_unifrac_pc.txt\n\n\n\n\n\n\nIn each file every sample is listed in the first column, and the\nsubsequent columns contain the value for the sample against the noted\nprincipal coordinate. At the bottom of each Principal Coordinate column,\nyou will find the eigen value and percent of variation explained by the\ncoordinate. Note the major difference between weighted and unweighted\nanalysis is the inclusion of OTU abundance when calculating distances\nbetween communities. You should use weighted if the biological question\nyou are trying to ask takes OTU abundance of your groups into\nconsideration. If some samples are forming groups with weighted, then\nlikely the larger or smaller abundances of several OTUs are the primary\ndriving force in PCoA space, but when all OTUs are considered at equal\nabundance, these differences are lost (unweighted).\n\n\nNote discrete vs. continuous only has to do with coloring of the points,\nand not the calculation of UniFrac distances.\n\n\nEmperor\n\u00b6\n\n\nWe can now run Emperor to prepare the PCoA plots for visualisation.\n\n\n1\n2\n3\nmake_emperor.py -i wf_bdiv_even394/unweighted_unifrac_pc.txt -m sutton_mapping_file.txt \u2013b \"Source,Condition,Source&&Condition\" \u2013o emporer_output_unweighted\n\nls \u2013l emporer_output_unweighted\n\n\n\n\n\n\nRun the last make_emporer.py command with the weighted_unifrac, using\nthe same command. Make sure you rename the output directory to\n\u201cemporer_output_weighted\u201c. Look in the Emporer output directories and\nnote the index.html file outputs.\n\n\nTo view the index.html files created from the \u2018make_emperor.py\u2019 steps\nabove a browser like Firefox or Chrome can be used.\n\n\nImportant - To view the Emporer plots we need to switch from the VM to\nyour local machine. Open the Firefox browser and view the pre-computed\nEmporer directories available at the following URL.\n\nhttp://www.ebi.ac.uk/~sterk/emperor/\n\n\nView the unweighted analysis on the Sutton sub data set:\n\n\nhttp://www.ebi.ac.uk/~sterk/emperor/sutton_subset_emporer_output_unweighted/\n\n\nCheck the \u2018Colors\u2019 options to view your samples. What can you determine\nfrom the PCoA plot?\n\n\nTo change the colouring scheme click the \u2018Colors\u2019 tab. The available\nmetadata categories are sorted in alphabetical order. Extra information\non the features can be found at\n\nhttp://qiime.org/emperor/description_index.html\n Try changing the\nscaling, visibility, label etc. to improve the display. Note that\nEmperor generates publication quality figures in scalable vector\ngraphics (SVG) format.\n\n\nNow look at the weighted unifrac analysis,\n\nhttp://www.ebi.ac.uk/~sterk/emperor/sutton_subset_emporer_output_weighted/\n\n\nYou can compare the sub data analysis against the full-denoised dataset.\n\n\n\nWhat are the main differences between the weighted and unweighted plots?\n\n\nDid you notice any differences between the analysis on the sub data set\nand the full denoised?\n\n\nWhat are your final conclusions on the Sutton dataset?\n\n\nWhat did each of the visualisation methods describe?\n\n\nTaxonomy summary plots:\n\n\nAlpha diversity plots:\n\n\nBeta diversity PCoA plots:\n\n\nNow that you are familiar with Emperor we will next view the forensic\nanalysis of the Fierer et. al. data. This experiment matches the\nbacteria on surfaces to the skin-associated bacteria of the individual\nwho touched the surface. This study shows that skin-associated bacteria\ncan be readily recovered from surfaces (including single computer keys\nand computer mice) and that the structure of these communities can be\nused to differentiate objects handled by different individuals, even if\nthose objects have been left untouched for up to 2 weeks at room\ntemperature.\n\n\nPlease look the information contained in the mapping file and identify\nthose fields that can be plotted.\n\n\n1\n2\ncd ~/Desktop/Taxonomy/fierer/\nless \u2013S fierer_mapping_file.txt\n\n\n\n\n\n\nList any fields that can be plotted for this study. What is the\nremainder of the information for?\n\n\nRun Emperor to prepare the PCoA plots for visualisation\n\n\n1\nmake_emperor.py -i fierer_unweighted_unifrac_pc.txt -m fierer_mapping_file.txt -b \\ \"HOST_SUBJECT_ID,ENV_FEATURE,HOST_SUBJECT_ID&&ENV_FEATURE\u201d -o emporer_output_unweighted\n\n\n\n\n\n\nGo to\n\nhttp://www.ebi.ac.uk/~sterk/emperor/fierer_emporer_output_unweighted/\n\n\nWhat did you observe about the clustering?\n\n\nWhat fields have been selected?\n\n\nThe on-line Qiime tutorials are very good and growing in numbers. It is\nhighly recommended checking out in future the many analysis options.",
            "title": "Metagenomics Visualisation"
        },
        {
            "location": "/modules/metagenomics-module-vis/vis/#analysis-visualisation",
            "text": "",
            "title": "Analysis Visualisation"
        },
        {
            "location": "/modules/metagenomics-module-vis/vis/#key-learning-outcomes",
            "text": "After completing this module the trainee should be able to:    Visualise between sample comparisons, using Emperor PCA    Understand the difference between weighted and unweighted analysis",
            "title": "Key Learning Outcomes"
        },
        {
            "location": "/modules/metagenomics-module-vis/vis/#resources-youll-be-using",
            "text": "",
            "title": "Resources You\u2019ll be Using"
        },
        {
            "location": "/modules/metagenomics-module-vis/vis/#tools-used",
            "text": "Emperor:   http://qiime.org/emperor/tutorial_index.html",
            "title": "Tools Used"
        },
        {
            "location": "/modules/metagenomics-module-vis/vis/#data-sets",
            "text": "Sutton et al. (2013). Impact of Long-Term Diesel Contamination on Soil Microbial Community Structure:   http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3553749/pdf/zam619.pdf  Fierer et al. (2010). Forensic identification using skin bacterial communities:   http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2852011/",
            "title": "Data sets"
        },
        {
            "location": "/modules/metagenomics-module-vis/vis/#introduction",
            "text": "Good data quality and sample metadata is important for visualising\nmetagenomics analysis.  For this tutorial we are using Emperor a browser enabled scatter plot\nvisualisation tool. We will be using the Sutton and Fierer data sets for\nviewing the principal components analysis (PCoA) from the QIIME Beta\ndiversity analysis (16S).",
            "title": "Introduction"
        },
        {
            "location": "/modules/metagenomics-module-vis/vis/#beta-diversity-computation-and-plots",
            "text": "Beta diversity represents between sample comparisons based on their\ncomposition. The output of these comparisons is a square matrix where a\n\u201cdistance\u201d or dissimilarity is calculated between every pair of\ncommunity samples, reflecting the dissimilarity between those samples.\nThe data distance matrix can be then visualized with analyses such as\nPCoA and hierarchical clustering. PCoA is a technique that maps the\nsamples in the distance matrix to a set of axes that show the maximum\namount of variation explained. The principal coordinates can be plotted\nin two or three dimensions to provide an intuitive visualization of the\ndata structure to look at differences between the samples, and look for\nsimilarities by sample conditions.  The Beta diversity workflow you ran earlier contained a number of steps:    Rarifying the OTU table to remove sample heterogeneity. A rarified\n    OTU table should be used so that artificial diversity induced due to\n    different sampling effort is removed.    Calculating the Beta diversity, the unifrac weighted and unweighted\n    generated principal coorodinates are created here by default.    The following are the required options and inputs for Beta diversity\nanalysis:  1\n2\n3\n4\n5 1.  Option -i, OTU table (*.biom)\n2.  Option \u2013t, phylogeny tree (*.tre) from pick_de_novo_otus.py\n3.  Option \u2013m, the user-defined mapping file\n4.  Option \u2013o, the output directory\n5.  Option \u2013e, the number of sequences per sample (sequencing depth)   You do not need to run the following command, it may overwrite the\npre-computed analysis. This is the command you ran on the Sutton sub\ndataset.  1 beta_diversity_through_plots.py -i otus/otu_table.biom -m mapping.txt -o wf_bdiv_even122/ -t otus/rep_set.tre -e 122",
            "title": "Beta Diversity computation and plots"
        },
        {
            "location": "/modules/metagenomics-module-vis/vis/#prepare-the-environment",
            "text": "The data for this practical can be found in the Taxonomy directory on\nyour desktop. Go to the pre-computed Beta diversity analysis in the\nTaxonomy directory.  Open the Terminal and go to where the data is stored. Investigate the\ndirectories available.  1\n2 cd ~/Desktop/Taxonomy/sutton_full_denoised/\nls \u2013lhtr   Open the following files: wf_bdiv_even394/unweighted_unifrac_pc.txt\nand wf_bdiv_even394/weighted_unifrac_pc.txt using the \u2018less\u2019\ncommand. The option \u2013S with \u2018less\u2019 allows you to view files with\nunwrapped lines, to escape type q (for quit). To scroll left and right,\nuse the arrow keys.  1\n2 less \u2013S wf_bdiv_even394/unweighted_unifrac_pc.txt\nless \u2013S wf_bdiv_even394/weighted_unifrac_pc.txt   In each file every sample is listed in the first column, and the\nsubsequent columns contain the value for the sample against the noted\nprincipal coordinate. At the bottom of each Principal Coordinate column,\nyou will find the eigen value and percent of variation explained by the\ncoordinate. Note the major difference between weighted and unweighted\nanalysis is the inclusion of OTU abundance when calculating distances\nbetween communities. You should use weighted if the biological question\nyou are trying to ask takes OTU abundance of your groups into\nconsideration. If some samples are forming groups with weighted, then\nlikely the larger or smaller abundances of several OTUs are the primary\ndriving force in PCoA space, but when all OTUs are considered at equal\nabundance, these differences are lost (unweighted).  Note discrete vs. continuous only has to do with coloring of the points,\nand not the calculation of UniFrac distances.",
            "title": "Prepare the environment"
        },
        {
            "location": "/modules/metagenomics-module-vis/vis/#emperor",
            "text": "We can now run Emperor to prepare the PCoA plots for visualisation.  1\n2\n3 make_emperor.py -i wf_bdiv_even394/unweighted_unifrac_pc.txt -m sutton_mapping_file.txt \u2013b \"Source,Condition,Source&&Condition\" \u2013o emporer_output_unweighted\n\nls \u2013l emporer_output_unweighted   Run the last make_emporer.py command with the weighted_unifrac, using\nthe same command. Make sure you rename the output directory to\n\u201cemporer_output_weighted\u201c. Look in the Emporer output directories and\nnote the index.html file outputs.  To view the index.html files created from the \u2018make_emperor.py\u2019 steps\nabove a browser like Firefox or Chrome can be used.  Important - To view the Emporer plots we need to switch from the VM to\nyour local machine. Open the Firefox browser and view the pre-computed\nEmporer directories available at the following URL. http://www.ebi.ac.uk/~sterk/emperor/  View the unweighted analysis on the Sutton sub data set:  http://www.ebi.ac.uk/~sterk/emperor/sutton_subset_emporer_output_unweighted/  Check the \u2018Colors\u2019 options to view your samples. What can you determine\nfrom the PCoA plot?  To change the colouring scheme click the \u2018Colors\u2019 tab. The available\nmetadata categories are sorted in alphabetical order. Extra information\non the features can be found at http://qiime.org/emperor/description_index.html  Try changing the\nscaling, visibility, label etc. to improve the display. Note that\nEmperor generates publication quality figures in scalable vector\ngraphics (SVG) format.  Now look at the weighted unifrac analysis, http://www.ebi.ac.uk/~sterk/emperor/sutton_subset_emporer_output_weighted/  You can compare the sub data analysis against the full-denoised dataset.  What are the main differences between the weighted and unweighted plots?  Did you notice any differences between the analysis on the sub data set\nand the full denoised?  What are your final conclusions on the Sutton dataset?  What did each of the visualisation methods describe?  Taxonomy summary plots:  Alpha diversity plots:  Beta diversity PCoA plots:  Now that you are familiar with Emperor we will next view the forensic\nanalysis of the Fierer et. al. data. This experiment matches the\nbacteria on surfaces to the skin-associated bacteria of the individual\nwho touched the surface. This study shows that skin-associated bacteria\ncan be readily recovered from surfaces (including single computer keys\nand computer mice) and that the structure of these communities can be\nused to differentiate objects handled by different individuals, even if\nthose objects have been left untouched for up to 2 weeks at room\ntemperature.  Please look the information contained in the mapping file and identify\nthose fields that can be plotted.  1\n2 cd ~/Desktop/Taxonomy/fierer/\nless \u2013S fierer_mapping_file.txt   List any fields that can be plotted for this study. What is the\nremainder of the information for?  Run Emperor to prepare the PCoA plots for visualisation  1 make_emperor.py -i fierer_unweighted_unifrac_pc.txt -m fierer_mapping_file.txt -b \\ \"HOST_SUBJECT_ID,ENV_FEATURE,HOST_SUBJECT_ID&&ENV_FEATURE\u201d -o emporer_output_unweighted   Go to http://www.ebi.ac.uk/~sterk/emperor/fierer_emporer_output_unweighted/  What did you observe about the clustering?  What fields have been selected?  The on-line Qiime tutorials are very good and growing in numbers. It is\nhighly recommended checking out in future the many analysis options.",
            "title": "Emperor"
        },
        {
            "location": "/modules/metagenomics-module-wga/wga/",
            "text": "Whole Genome Analysis\n\u00b6\n\n\nKey Learning Outcomes\n\u00b6\n\n\n\n\nAfter completing this module the trainee should be able to:\n\n\n\n\n\n\nUnderstand the main approaches to perform metagenomics assembly\n\n\n\n\n\n\nBe able to perform assembly on your data and assess the quality of your assembly\n\n\n\n\n\n\nResources You\u2019ll be Using\n\u00b6\n\n\n\n\nTools Used\n\u00b6\n\n\nMeta-Velvet: \n\n\nhttps://github.com/hacchy/MetaVelvet\n\n\nUseful Links\n\u00b6\n\n\n\n\nMeta-Velvet: \n\n\nhttp://metavelvet.dna.bio.keio.ac.jp/\n\n\nIntroduction\n\u00b6\n\n\n\n\nPerforming genomic assembly aims at generating a genome-length sequence\nusing the sequence information obtained from short reads. In the case of\nmetagenomics sample, the task is complicated by the number of different\ngenomes present in the sample and the fact that their sequences are\nsometimes very similar to each other. There are two main approaches to\nperform de novo assembly (genomic or metagenomic): building a consensus\nand generating De Bruijn k-mer graph.\n\n\nThe k-mers represent the nodes of the de Bruijn graph. Nodes are linked\ntogether if they overlap by k-1 nucleotides. Determining the correct\nk-mer is important. You can use tools such as Velvet Advisor:\n\nhttp://dna.med.monash.edu.au/~torsten/velvet_advisor/\n\n\nBuilding a de Bruijn graph is computationally demanding but navigation\nthrough it to identify path (to generate contigs of continuous\nsequences) is quick and memory efficient. Ideally other information,\nbiological or distance-based, would be used to help build the contigs.\n\n\nAssessing the quality of an assembly\n\u00b6\n\n\nFor genomic assembly, the accepted criterion of assembly quality is the\nnumber of contigs obtained: the lower this number, the longer the\ncontigs and therefore the higher genome reconstitution. This number is\noften expressed as N50, which is defined as the weighted median such\nthat 50% of the entire assembly is contained in contigs equal to or\nlarger than this value. It is calculated by ranking the contigs by\ndecreasing length and adding their size sequentially until 50% of the\ntotal number of nucleotides is reached: the N50 is defined by the number\nof contigs included in this sum. The N50 is also generally used for\nmetagenomics\n\n\nPractical\n\u00b6\n\n\n\n\nThe purpose of this exercise is to perform an assembly using Meta-Velvet\nand illustrate how k-mer choices influence the output quality. The\nstarting dataset will be a metagenomic dataset. To ensure better\nassembly, rather than using the raw reads, we will only assemble the\nsequences having passed the EMG QC steps. Meta-Velvet is an extension of\nVelvet, a popular genomic assembler. Therefore to perform our assembly,\nwe will first run Velvet and then Meta-Velvet using the Velvet output as\ninput. Both programs are run from the command line.\n\n\nInvestigation of the input sequence file\n\u00b6\n\n\nOpen a terminal window (Applications/Accessories/Terminal, a link is\nalso provided on your desktop) and navigate to the \u201cdata\u201d folder in\n\u201cAssemblyTutorial\u201d and look at the first lines of the sequence file. The\nfile is in fasta format and contains sequences of at least 100 nt each\n\n\n1\n2\n3\n4\n5\n6\ncd ~/Desktop/AssemblyTutorial/data\nhead A7A_processed.fasta\n# What is the total number of sequences?\ngrep -c \">\" A7A_processed.fasta\n# What is the total number of nucleotides?\n~/AssemblyTutorial/stats A7A_processed.fasta\n\n\n\n\n\n\nThe output indicates that the input file contains about 2.2 billion\n(2,164,714,530) nucleotides distributed among \u00a0 21 million (20,975,212)\nsequences. The average sequence length was 103.2 nucleotides. The\nsequence file also contains 12,942 \u201cN\u201d indicating that some sequences\nhave ambiguous bases. In addition, the script displays the N50 to N100\nvalues: N50 = 101, n = 10256045 for example means that a cumulated sum\nof, at least, half of the total nucleotides is reached after adding the\nlength of 10,256,045 sequences and that the last sequence added had a\nlength of 101 nucletotides.\n\n\nPerforming the assembly using Velvet and Meta-Velvet\n\u00b6\n\n\nVelvet and Meta-Velvet had already been installed on your computer.\nHowever they need to be configured by indicating the k-mer value and the\nnumber of read categories to use. We already have seen what k-mer is\n(reminder in page 5 above). The number of read categories is the maximum\nnumber of libraries of different insert lengths. As in our case the\nreads are all coming from the same library, we will use the default\nvalue (2). The version of Velvet and MetaVelvet installed on the virtual\nmachine you will be using as already been configured with k-mer = 59.\n\n\nWe will run the applications from the AssemblyTutorial folder. The\nsoftware has been installed in your path so no need to copy/link these\nfiles:\n\n\n1\n2\n# first run velveth to generate the k-mers:\nvelveth A7A-59 59 -fasta data/A7A_processed.fasta\n\n\n\n\n\n\nThen run velvetg to construct the de Bruijn graph. The \u201c- exp_cov auto\u201d\nparameter indicates to the software that the sequence coverage is\nconsidered uniform across the submitted set and that the expected\ncoverage (i.e. number of reads per sequence) should be the median\ncoverage value:\n\n\n1\nvelvetg A7A-59 \u2013exp_cov auto\n\n\n\n\n\n\nFinally run meta-velvetg to generate the assembly output in the A7A-59\ndirectory:\n\n\n1\nmeta-velvetg A7A-59 | tee logfile\n\n\n\n\n\n\nAssessing the quality of the assembly\n\u00b6\n\n\nThe main assembly output is a list of contigs provided as a fasta file.\nWe will know look at these in more details. First we need to navigate to\nthe output directory:\n\n\nFinally run meta-velvetg to generate the assembly output in the A7A-59\ndirectory. We can obtain the number of contigs by running the function\ngrep to only count the lines containing the contig names (identified by\n\u201c>\u201d). Then run the stats script, seen earlier, to also obtain the N50\nvalue:\n\n\n1\n2\n3\ncd A7A-59\ngrep -c \">\" meta-velvetg.contigs.fa\n~/AssemblyTutorial/stats meta-velvetg.contigs.fa\n\n\n\n\n\n\nIt shows that the sequences had been assembled in 9,182 contigs of an\naverage length equal to about 1,230 nucleotides. The longest contigs\ncontains 95,305 nucleotides. The N50 line indicates that half of the\nnucleotides are comprised in the first 275 longest contigs and that the\n275\nth\n contigs is 9,145 nucleotides long. Comparing these stats to the\none obtained before assembly also reveal that the number of nucleotides\ninvolved in the assembly represents only slightly more than 0.5% of the\nnucleotides submitted. This reflects, of course, the amount of\noverlapping sequences identified by Velvet and MetaVelvet. This also\nexplains the reduction of the number of ambiguous base (N_count).\n\n\nChanging the k-mer value can have a dramatic effect on the quality of\nthe assembly. Reducing the k-mer to 31 for example yields the following\nstatistics:\n\n\n1\n2\n3\n4\n5\n6\n7\n8\nsum = 15527668, n = 42343, ave = 366.71, largest = 93835\nN50 = 1208, n = 2151\nN60 = 649, n = 3903\nN70 = 328, n = 7388\nN80 = 193, n = 13589\nN90 = 115, n = 24202\nN100 = 61, n = 42343\nN_count = 263\n\n\n\n\n\n\nThe number of contigs is almost 5 times larger than with k-mer equal 59.\nHowever, increasing the k-mer alone does not systematically lead to\nbetter stats. A k-mer of 63 produces an assembly with higher number of\ncontigs (Note that the N50 value is also increased):\n\n\n1\n2\n3\n4\n5\n6\n7\n8\nsum = 11579884, n = 10062, ave = 1150.85, largest = 80291\nN50 = 7440, n = 365\nN60 = 4524, n = 563\nN70 = 2251, n = 921\nN80 = 827, n = 1795\nN90 = 326, n = 4155\nN100 = 125, n = 10062\nN_count = 1850\n\n\n\n\n\n\nWithout extra information, it could be challenging, using the N50\nparameter, to judge the quality of a metagenomics assembly. We could use\nblast or other tools to infer taxonomy to different sections of the\ncontigs: obtaining similar affiliation for all fragments originating\nfrom the same contigs would be indicative of a good assembly.",
            "title": "Metagenomics WGA"
        },
        {
            "location": "/modules/metagenomics-module-wga/wga/#whole-genome-analysis",
            "text": "",
            "title": "Whole Genome Analysis"
        },
        {
            "location": "/modules/metagenomics-module-wga/wga/#key-learning-outcomes",
            "text": "After completing this module the trainee should be able to:    Understand the main approaches to perform metagenomics assembly    Be able to perform assembly on your data and assess the quality of your assembly",
            "title": "Key Learning Outcomes"
        },
        {
            "location": "/modules/metagenomics-module-wga/wga/#resources-youll-be-using",
            "text": "",
            "title": "Resources You\u2019ll be Using"
        },
        {
            "location": "/modules/metagenomics-module-wga/wga/#tools-used",
            "text": "Meta-Velvet:   https://github.com/hacchy/MetaVelvet",
            "title": "Tools Used"
        },
        {
            "location": "/modules/metagenomics-module-wga/wga/#useful-links",
            "text": "Meta-Velvet:   http://metavelvet.dna.bio.keio.ac.jp/",
            "title": "Useful Links"
        },
        {
            "location": "/modules/metagenomics-module-wga/wga/#introduction",
            "text": "Performing genomic assembly aims at generating a genome-length sequence\nusing the sequence information obtained from short reads. In the case of\nmetagenomics sample, the task is complicated by the number of different\ngenomes present in the sample and the fact that their sequences are\nsometimes very similar to each other. There are two main approaches to\nperform de novo assembly (genomic or metagenomic): building a consensus\nand generating De Bruijn k-mer graph.  The k-mers represent the nodes of the de Bruijn graph. Nodes are linked\ntogether if they overlap by k-1 nucleotides. Determining the correct\nk-mer is important. You can use tools such as Velvet Advisor: http://dna.med.monash.edu.au/~torsten/velvet_advisor/  Building a de Bruijn graph is computationally demanding but navigation\nthrough it to identify path (to generate contigs of continuous\nsequences) is quick and memory efficient. Ideally other information,\nbiological or distance-based, would be used to help build the contigs.",
            "title": "Introduction"
        },
        {
            "location": "/modules/metagenomics-module-wga/wga/#assessing-the-quality-of-an-assembly",
            "text": "For genomic assembly, the accepted criterion of assembly quality is the\nnumber of contigs obtained: the lower this number, the longer the\ncontigs and therefore the higher genome reconstitution. This number is\noften expressed as N50, which is defined as the weighted median such\nthat 50% of the entire assembly is contained in contigs equal to or\nlarger than this value. It is calculated by ranking the contigs by\ndecreasing length and adding their size sequentially until 50% of the\ntotal number of nucleotides is reached: the N50 is defined by the number\nof contigs included in this sum. The N50 is also generally used for\nmetagenomics",
            "title": "Assessing the quality of an assembly"
        },
        {
            "location": "/modules/metagenomics-module-wga/wga/#practical",
            "text": "The purpose of this exercise is to perform an assembly using Meta-Velvet\nand illustrate how k-mer choices influence the output quality. The\nstarting dataset will be a metagenomic dataset. To ensure better\nassembly, rather than using the raw reads, we will only assemble the\nsequences having passed the EMG QC steps. Meta-Velvet is an extension of\nVelvet, a popular genomic assembler. Therefore to perform our assembly,\nwe will first run Velvet and then Meta-Velvet using the Velvet output as\ninput. Both programs are run from the command line.",
            "title": "Practical"
        },
        {
            "location": "/modules/metagenomics-module-wga/wga/#investigation-of-the-input-sequence-file",
            "text": "Open a terminal window (Applications/Accessories/Terminal, a link is\nalso provided on your desktop) and navigate to the \u201cdata\u201d folder in\n\u201cAssemblyTutorial\u201d and look at the first lines of the sequence file. The\nfile is in fasta format and contains sequences of at least 100 nt each  1\n2\n3\n4\n5\n6 cd ~/Desktop/AssemblyTutorial/data\nhead A7A_processed.fasta\n# What is the total number of sequences?\ngrep -c \">\" A7A_processed.fasta\n# What is the total number of nucleotides?\n~/AssemblyTutorial/stats A7A_processed.fasta   The output indicates that the input file contains about 2.2 billion\n(2,164,714,530) nucleotides distributed among \u00a0 21 million (20,975,212)\nsequences. The average sequence length was 103.2 nucleotides. The\nsequence file also contains 12,942 \u201cN\u201d indicating that some sequences\nhave ambiguous bases. In addition, the script displays the N50 to N100\nvalues: N50 = 101, n = 10256045 for example means that a cumulated sum\nof, at least, half of the total nucleotides is reached after adding the\nlength of 10,256,045 sequences and that the last sequence added had a\nlength of 101 nucletotides.",
            "title": "Investigation of the input sequence file"
        },
        {
            "location": "/modules/metagenomics-module-wga/wga/#performing-the-assembly-using-velvet-and-meta-velvet",
            "text": "Velvet and Meta-Velvet had already been installed on your computer.\nHowever they need to be configured by indicating the k-mer value and the\nnumber of read categories to use. We already have seen what k-mer is\n(reminder in page 5 above). The number of read categories is the maximum\nnumber of libraries of different insert lengths. As in our case the\nreads are all coming from the same library, we will use the default\nvalue (2). The version of Velvet and MetaVelvet installed on the virtual\nmachine you will be using as already been configured with k-mer = 59.  We will run the applications from the AssemblyTutorial folder. The\nsoftware has been installed in your path so no need to copy/link these\nfiles:  1\n2 # first run velveth to generate the k-mers:\nvelveth A7A-59 59 -fasta data/A7A_processed.fasta   Then run velvetg to construct the de Bruijn graph. The \u201c- exp_cov auto\u201d\nparameter indicates to the software that the sequence coverage is\nconsidered uniform across the submitted set and that the expected\ncoverage (i.e. number of reads per sequence) should be the median\ncoverage value:  1 velvetg A7A-59 \u2013exp_cov auto   Finally run meta-velvetg to generate the assembly output in the A7A-59\ndirectory:  1 meta-velvetg A7A-59 | tee logfile",
            "title": "Performing the assembly using Velvet and Meta-Velvet"
        },
        {
            "location": "/modules/metagenomics-module-wga/wga/#assessing-the-quality-of-the-assembly",
            "text": "The main assembly output is a list of contigs provided as a fasta file.\nWe will know look at these in more details. First we need to navigate to\nthe output directory:  Finally run meta-velvetg to generate the assembly output in the A7A-59\ndirectory. We can obtain the number of contigs by running the function\ngrep to only count the lines containing the contig names (identified by\n\u201c>\u201d). Then run the stats script, seen earlier, to also obtain the N50\nvalue:  1\n2\n3 cd A7A-59\ngrep -c \">\" meta-velvetg.contigs.fa\n~/AssemblyTutorial/stats meta-velvetg.contigs.fa   It shows that the sequences had been assembled in 9,182 contigs of an\naverage length equal to about 1,230 nucleotides. The longest contigs\ncontains 95,305 nucleotides. The N50 line indicates that half of the\nnucleotides are comprised in the first 275 longest contigs and that the\n275 th  contigs is 9,145 nucleotides long. Comparing these stats to the\none obtained before assembly also reveal that the number of nucleotides\ninvolved in the assembly represents only slightly more than 0.5% of the\nnucleotides submitted. This reflects, of course, the amount of\noverlapping sequences identified by Velvet and MetaVelvet. This also\nexplains the reduction of the number of ambiguous base (N_count).  Changing the k-mer value can have a dramatic effect on the quality of\nthe assembly. Reducing the k-mer to 31 for example yields the following\nstatistics:  1\n2\n3\n4\n5\n6\n7\n8 sum = 15527668, n = 42343, ave = 366.71, largest = 93835\nN50 = 1208, n = 2151\nN60 = 649, n = 3903\nN70 = 328, n = 7388\nN80 = 193, n = 13589\nN90 = 115, n = 24202\nN100 = 61, n = 42343\nN_count = 263   The number of contigs is almost 5 times larger than with k-mer equal 59.\nHowever, increasing the k-mer alone does not systematically lead to\nbetter stats. A k-mer of 63 produces an assembly with higher number of\ncontigs (Note that the N50 value is also increased):  1\n2\n3\n4\n5\n6\n7\n8 sum = 11579884, n = 10062, ave = 1150.85, largest = 80291\nN50 = 7440, n = 365\nN60 = 4524, n = 563\nN70 = 2251, n = 921\nN80 = 827, n = 1795\nN90 = 326, n = 4155\nN100 = 125, n = 10062\nN_count = 1850   Without extra information, it could be challenging, using the N50\nparameter, to judge the quality of a metagenomics assembly. We could use\nblast or other tools to infer taxonomy to different sections of the\ncontigs: obtaining similar affiliation for all fragments originating\nfrom the same contigs would be indicative of a good assembly.",
            "title": "Assessing the quality of the assembly"
        },
        {
            "location": "/modules/metagenomics-module-fda/fda/",
            "text": "Functional Data Analysis\n\u00b6\n\n\nKey Learning Outcomes\n\u00b6\n\n\n\n\nAfter completing this module the trainee should be able to:\n\n\n\n\n\n\nUnderstand how EMG provides functional analysis of metagenomic data sets\n\n\n\n\n\n\nKnow where to find and how to interpret analysis results for samples on the EMG website\n\n\n\n\n\n\nKnow how to download the raw sample data and analysis results for use with 3\nrd\n party visualisation and statistical analysis packages\n\n\n\n\n\n\nResources You\u2019ll be Using\n\u00b6\n\n\n\n\nTools Used\n\u00b6\n\n\nMegan6 :  \nhttp://ab.inf.uni-tuebingen.de/software/megan6/\n\n\nUseful Links\n\u00b6\n\n\n\n\nEBI Metagenomics resource (EMG) : \n\n\nwww.ebi.ac.uk/metagenomics/\n\n\nIntroduction\n\u00b6\n\n\n\n\nThe EBI Metagenomics resource (EMG) provides functional analysis of predicted coding sequences (pCDS) from metagenomic data sets using the InterPro database. InterPro is a sequence analysis resource that predicts protein family membership, along with the presence of important domains and sites. It does this by combining predictive models known as protein signatures from a number of different databases into a single searchable resource. InterPro curators manually integrate the different signatures, providing names and descriptive abstracts and, whenever possible, adding Gene Ontology (GO) terms. Links are also provided to pathway databases, such as KEGG, MetaCyc and Reactome, and to structural resources, such as SCOP, CATH and PDB.\n\n\nWhat are protein signatures?\n\u00b6\n\n\nProtein signatures are obtained by modelling the conservation of amino acids at specific positions within a group of related proteins (i.e., a protein family), or within the domains/sites shared by a group of proteins. InterPro\u2019s different member databases use different computational methods to produce protein signatures, and they each have their own particular focus of interest: structural and/or functional domains, protein families, or protein features, such as active sites or binding sites (see Figure 1).\n\n\n\n\nFigure 1. InterPro member databases grouped by the methods, indicated in white coloured text, used to construct their signatures. Their focus of interest is shown in blue text.\n\n\nOnly a subset of the InterPro member databases are used by EMG: Gene3D, TIGRFAMs, Pfam, PRINTS and PROSITE patterns. These databases were selected since, together, they provide both high coverage and offer detailed functional analysis, and have underlying algorithms that can cope with the vast amounts of fragmentary sequence data found in metagenomic datasets.\n\n\nAssigning functional information to metagenomic sequences\n\u00b6\n\n\nWhilst InterPro matches to metagenomic sequence sets are informative in their own right, EMG offers an additional type of analysis in the form of Gene Ontology (GO) terms. The Gene Ontology is made up of 3 structured controlled vocabularies that describe gene products in terms of their associated biological processes, cellular components and molecular functions in a species-independent manner. By using GO terms, scientists working on different species or using different databases can compare datasets, since they have a precisely defined name and meaning\nfor a particular concept. Terms in the Gene Ontology are ordered into hierarchies, with less specific terms towards the top and more specific terms towards the bottom (see Figure 2).\n\n\n\n\nFigure 2. An example of GO terms organised into a hierarchy, with terms becoming less specific as the hierarchy is ascended (e.g., alpha-tubulin binding is a type of cytoskeletal binding, which is a type of protein binding). Note that a GO term can have more than one parent term. The Gene Ontology also allows for different types of relationships between terms (such as \u2018has part of\u2019 or \u2018regulates\u2019). The EMG analysis pipeline only uses the straightforward \u2018is a\u2019 relationships.\n\n\nMore information about the GO project can be found\n\nhttp://www.geneontology.org/GO.doc.shtml\n\n\nAs part of the EMG analysis pipeline, GO terms for molecular function,biological process and cellular component are associated to pCDS in a sample via the InterPro2GO mapping service. This works as follows:\nInterPro entries are given GO terms by curators if the terms can be accurately applied to all of the proteins matching that entry. Sequences searched against InterPro are then associated with GO terms by virtue of the entries they match - a protein that matches one InterPro entry with the GO term \u2018kinase activity\u2019 and another InterPro entry with the GO term \u2018zinc ion binding\u2019 will be annotated with both GO terms.\n\n\nFinding functional information about samples on the EMG website\n\u00b6\n\n\nFunctional analysis of samples within projects on the EMG website\n\nwww.ebi.ac.uk/metagenomics/\n can be accessed by clicking on the Functional Analysis tab found toward the top of any sample page (see Figure 3 below).\n\n\n\n\nFigure 3. A Functional analysis tab can be found towards the top of each run page\n\n\nClicking on this tab brings up a page displaying sequence features (the number of reads with pCDS, the number of pCDS with InterPro matches, etc), InterPro match information and GO term annotation for the sample, as shown in Figure 4 and 5 below.\n\n\nInterPro match information for the predicted coding sequences in the sample is shown. The number of InterPro matches are displayed graphically, and as a table that has a text search facility.\n\n\n\n\nFigure 4. Functional analysis of metagenomics data, as shown on the EMG website.\n\n\n\n\nFigure 5. The GO terms predicted for the sample are displayed. Different graphical representations are available, and can be selected by clicking on the \u2018Switch view\u2019 icons.\n\n\nThe Gene Ontology terms displayed graphically on the web site have been \u2018slimmed\u2019 with a special GO slim developed for metagenomic data sets. GO slims are cut-down versions of the Gene Ontology, containing a subset of the terms in the whole GO. They give a broad overview of the ontology content without the detail of the specific fine-grained terms.\n\n\nThe full data sets used to generate both the InterPro and GO overview charts, along with a host of additional data (processed reads, pCDS, reads encoding 16S rRNAs, taxonomic analyses, etc) can be downloaded for further analysis by clicking the Download tab, found towards the top of the page (see Figure 6).\n\n\n\n\nFigure 6. Each sample has a download tab, where the full set of sequences, analyses, summaries and raw data can be downloaded.\n\n\nPractical\n\u00b6\n\n\n\n\nBrowsing analysed data via the EMG website\n\u00b6\n\n\nFor this session, we are going to look at the Ocean Sampling Day (OSD) 2014 project, which involved simultaneously sampling from geographically diverse oceanographic sites on Solstice 2014. A map of all of the sampling sites is shown on the project page: \nhttps://www.ebi.ac.uk/metagenomics/projects/ERP009703\n\n\nTo get the OSD project page, either follow the above link or open the Metagenomics Portal home page (\nhttps://www.ebi.ac.uk/metagenomics/\n).\n\n\n\n\nenter \u2018\nOSD\n\u2019 in the search box on the top right hand side of the page, and follow the link to project \nERP009703\n. You should now have a Project overview page, which describes the project, submitter contact details, and links to the samples and runs that the project contains.\n\n\nScroll down to Associated Runs and use the \u2018\nFilter\n\u2019 search box to find the OSD80_2014-06-21_0m_NPL022 sample (\nERS667582\n). \n\n\nClick on the Sample Name link (not the Run link) to arrive at the overview page, describing various contextual data, such as the geographic location from which the material was isolated, its collection date, and so on.\n\n\n\n\n\n\nQuestion 1:\n\n\nWhat is the latitude, longitude and depth at which the sample was collected?\n\n\n\n\n\n\nQuestion 2:\n\n\nWhat geographic location does this correspond to?\n\n\n\n\n\n\nQuestion 3:\n\n\nWhat environmental ontology (ENVO) identifer and name has the sample material been annotated with?\n\n\n\n\nNow scroll down to the \u2018Associated runs\u2019 section of the page. Some samples can have a number of sequencing runs associated with them (for example, corresponding to 16S rRNA amplicon analyses and WGS analyses performed on the same sample). In this case, there is only 1 associated run: ERR770971. Click on the Run ID to go to the Run page.\n\n\nThis page has a number of tabs towards the top (Overview, Quality control, Taxonomy analysis, Functional analysis, and Download). Click on the \u2018Download\u2019 tab. Click the file labelled \u2018\nPredicted CDS\n\u2019 link to save this file to your computer. Find the file (it should be in your Downloads folder), unzip it and examine it using \u2018less\u2019 by typing the following commands in a terminal window:\n\n\nClick on the \u2018Download\u2019 tab. Right click the file labelled \u2018Predicted CDS (FASTA)\u2019 link, and save this file to your desktop. Find the file, and either double click on it to open it, or examine it using \u2018less\u2019 by typing the following commands in a terminal window:\n\n\n1\n2\n3\ncd ~/Downloads\ngzip \u2013d ERR770971_MERGED_FASTQ_pCDS.faa.gz \nless ERR770971_MERGED_FASTQ_pCDS.faa\n\n\n\n\n\n\nHave a look at one or two of the many sequences it contains. \n\n\nYou can count the total number of sequences in the file by grepping the number of header lines that start with \u201c>\u201d\n\n\n1\ngrep -c \n\"^> \"\nERR770971_MERGED_FASTQ_pCDS.faa\n\n\n\n\n\n\nIn a moment, we will look at the analysis results for this entire batch of sequences, displayed on the EMG website. First, we will attempt to analyse just one of the predicted coding sequences using InterPro (the analysis results on the EMG website summarise these kind of results for hundreds of thousands of sequences).\n\n\nIn a new tab or window, open your web browser and navigate to \nhttp://www.ebi.ac.uk/interpro/\n. Copy and paste the following sequence into the text box on the InterPro home page where it says \u2018Analyse your sequence\u2019:\n\n\n1\n2\n>HWI-M02024:110:000000000-A8H0K:1:1101:23198:21331-1:N:0:TCAGAGAC_1_267\nHLLSYRYAYGKFSSTHEATIGGCFLTKDEELDDHIVKYEIWDTAGKNGTIHLPRCTTSKAYXIQVTWYRNAIAAVVVFDVTSRDSFEK\n\n\n\n\n\n\nPress Search and wait for your results. Your sequence will be run through the InterProScan analysis software, which attempts to match it against all of the different signatures in the InterProScan database.\n\n\n\n\nQuestion 4:\n\n\nWhich protein family does InterProScan predict your sequence belongs to, and what GO terms are predicted to describe its function?\n\n\n\n\nClicking on the InterPro entry name or IPR accession number will take you to the InterPro entry page for your result, where more information\ncan be found.\n\n\nReturn to the overview page for \nERR770971\n.\n\n\nNow we are going to look at the functional analysis results for all of the pCDS in the sample. First, we will find the number of sequences that made it through to the functional analysis section of the pipeline.\n\n\nClick on the Quality control tab. This page displays a series of charts, showing how many sequences passed each quality control step, how many reads were left after clustering, and so on.\n\n\n\n\nQuestion 5:\n\n\nAfter all of the quality filtering steps are complete, how many reads were submitted for analysis by the pipeline?\n\n\n\n\nNext, we will look at the results of the functional predictions for the\npCDS. These can be found under the Functional analysis tab.\n\n\nClick on the Functional analysis tab and examine the InterPro match section. The top part of this page shows a sequence feature summary, showing the number of reads with predicted coding sequences (pCDS), the number of pCDS with InterPro matches, etc.\n\n\n\n\nQuestion 6:\n\n\nHow many predicted coding sequences (pCDS) are in the run?\n\n\n\n\n\n\nQuestion 7:\n\n\nHow many pCDS have InterProScan hits?\n\n\n\n\nScroll down the page to the InterPro match summary section\n\n\n\n\nQuestion 8:\n\n\nHow many different InterPro entries were matched by the pCDS?\n\n\n\n\n\n\nQuestion 9:\n\n\nWhy is this figure different to the number of pCDS that have InterProScan hits?\n\n\n\n\nNext we will examine the GO terms predicted by InterPro for the pCDS in the sample.\n\n\nScroll down to the GO term annotation section of the page and examine the 3 bar charts, which correspond to the 3 different components of the\nGene Ontology.\n\n\n\n\nQuestion 10:\n\n\nWhat are the top 3 biological process terms predicted for the pCDS from the sample?\n\n\n\n\nSelecting the pie chart representation of GO terms makes it easier to visualise the data to find the answer.\n\n\nNow we will look at the taxonomic analysis for this run.\n\n\nClick on the Taxonomy Analysis tab and examine the phylum composition graph and table.\n\n\n\n\nQuestion 11:\n\n\nHow many of the WGS reads are predicted to encode 16S rRNAs?\n\n\n\n\n\n\nQuestion 12:\n\n\nWhat are the top 3 phyla in the run, according to 16S rRNA analysis?\n\n\n\n\nSelect the Krona chart view of the data icon. This brings up an\ninteractive chart that can be used to analyse data at different\ntaxonomic ranks.\n\n\n\n\nQuestion 13:\n\n\nWhat is the proportion of Polaribacter in the population?\n\n\n\n\n\n\nNote\n\n\nIf the cyanobacteria section of the chart looks strange, this is because the version of GreenGenes used for analysis lists chloroplastic organisms under the cyanobacteria category; some of the cyanobacterial counts are, in fact, derived from photosynthetic eukaryotic organisms.\n\n\n\n\nNow we will compare these analyses with those for a sample taken at 2 m depth from the same geographical location.\n\n\nIn a new tab or window, find and open the Ocean Sampling Day (OSD) 2014 project page again. Find the sample OSD80_2014-06-21_2m_NPL022 and examine metadata on the Overview page.\n\n\n\n\nQuestion 14:\n\n\nOther than sampling depth, what are the differences between this sample and OSD80_2014-06-21_0m_NPL02?\n\n\n\n\nScroll to the Assocated runs section, and click on ERR770970. Open the Functional analysis tab and examine the Sequence feature and InterPro match summary information for this run.\n\n\n\n\nQuestion 15:\n\n\nHow many pCDS were in this run? \n\n\n\n\n\n\nQuestion 16:\n\n\nHow many of the pCDSs have an InterPro match?\n\n\n\n\n\n\nQuestion 17:\n\n\nHow many different InterPro entries are matched by this run?\n\n\n\n\n\n\nQuestion 18:\n\n\nAre these figures broadly comparable to those for the previous sample?\n\n\n\n\nNow we are going to look at the differences in slimmed GO terms between the 2 runs. There are to ways to do this. First, you can simply scroll to the bottom of the page and examine the GO term annotation (note - selecting the bar chart representation of GO terms makes it easier to compare different data sets). Alternatively, you can use the comparison tool, which allows direct comparison of runs within a project. The tool can be accessed by clicking on the \u2018Comparison Tool\u2019 tab, illustrated in Figure 6 below. At present, the tool only compares slimmed GO terms, but will be expanded to cover full GO terms, InterPro annotations, and taxonomic profiles as development of the site continues.\n\n\nClick on the Comparison tool tab and choose the Ocean Sampling Day (OSD) 2014 project from the sample list and select the OSD80_2014-06- 21_0m_NPL022 - ERR770971 and OSD80_2014-06-21_2m_NPL022 - ERR770970.\n\n\n\n\nQuestion 19:\n\n\nAre there visible differences between the GO terms for these runs. Could there be any biological explanation for this?\n\n\n\n\nNavigate back and open the taxonomic analysis results tab for each run.\n\n\n\n\nQuestion 20:\n\n\nHow does the taxonomic composition differ between runs? Are any trends in the data consistent with your answer to question 19?\n\n\n\n\nVisualising taxonomic data using MEGAN\n\u00b6\n\n\nNext, we are going to look at the taxonomic predictions for all of the runs. To do this, we are going to load them into MEGAN. MEGAN is a tool suite that provides metagenomic data analysis and visualization. We are going to use only a small subset of its features, relating to taxonomic comparison. Detailed information on MEGAN and the analyses and visualisations it offers can be found here: \n\n1\nhttp://ab.inf.uni-tuebingen.de/data/software/megan5/download/manual.pdf\n\n\n\n\n\nMEGAN can be downloaded from \n\n1\nhttp://ab.inf.uni-tuebingen.de/software/megan6/\n\n\n\n\n(there are 2 editions Community and Ultimate. The Community edition is open source and free to download. We will use this version for the tutorial). However, it should already be installed on your Desktop.\n\n\nClick\n on the MEGAN icon on your desktop to load the s/w.\n\n\nWe now need to download the full taxonomic predictions for all of the runs in the Ocean Sampling Day project.\n\n\nNavigate to the Project: Ocean Sampling Day (OSD) page, using the breadcrumb URL link at the top of the page. Click on the Analysis summary tab, which will take you to a set of tab separated result matrix files, summarising the taxonomic and functional observations for all runs in the project.\n\n\nClick on the Taxonomic assignments (TSV) link, which will download the corresponding file to your computer. When prompted, choose to save the file in the Downloads folder.\n\n\nOpen the Terminal, navigate to the Downloads directory and take a look at the file you have just downloaded (hit q to exit less):\n\n\n1\n2\ncd\n ~/Downloads\nless -S ERP009703_taxonomy_abundances_v2.0.tsv\n\n\n\n\n\n\nYou will see it is a large matrix file, with abundance counts for each taxonomic lineage for each run in the project. From the MEGAN menu, choose \u2018File\u2019 and \u2018Import\u2019. Select \u2018CSV Import\u2019 and then find the file you have just downloaded and edited.\n\n\nFrom the pop up menu, go with the default options in the Format and Separator sections (which should have the \u2018Class, Count\u2019 option selected under format, and the separator set as \u2018Tab\u2019) and select \u2018Taxonomy\u2019 in classification section, then press \u2018Apply\u2019.\n\n\nThere are many different visualisations and comparisons that can be performed using MEGAN, so feel free to explore the data using the tool.\n\n\nClick on the \u2018Show chart\u2019 icon and choose \u2018Stacked Bar Chart\u2019. This should give you a bar chart, showing the abundance reads for taxonomic lineages across all of the sampling sites.\n\n\n\n\nQuestion 21:\n\n\nAre the number of classified 16S reads roughly equivalent across all of the different sampling sites?\n\n\n\n\n\n\nQuestion 22:\n\n\nWhich run appears particularly enriched in Polaribacter?\n\n\n\n\nYou can change between abundance counts and relative abundance using the Options drop menu and choosing % Percentage Scale or Linear Scale.\n\n\nChange the chart view to \u2018Bubble Chart\u2019. This visualisation can be useful when comparing a large number of samples.\n\n\n\n\nQuestion 23:\n\n\nDo any samples contain taxa not found in other samples? \n\n\n\n\nTake a look at the Schlegelella genus.\n\n\n\n\nQuestion 24:\n\n\nCan you discern any patterns in the geographical distribution of certain species (for example, the cluster of samples enriched for the lactobacillus genus compared to other samples)?\n\n\n\n\nWe are now going to take a look at which other datasets in EBI Metagenomics that lactobacilli are found in. Point your browser at \nhttps://www.ebi.ac.uk/metagenomics/search/\n\n\nThis interface allows you to search the project, sample and run related metadata and analysis results for all of the publicly available datasets in the EBI Metagenomics resource.\n\n\nClick on the \u2018Runs\u2019 tab. You should now see a number of run-related metadata search facets on the left hand side of the page, including \u2018Organism\u2019. Click on the \u2018More\u2026\u2019 option under Organism, scroll down to \u2018Lactobacillus\u2019 in the pop-up window and select the check box next to it. Now click \u2018Filter\u2019. The results page should now show all of the runs that have taxonomic matches to lactobacilli in their datasets. The \u2018Biome\u2019 facet on the left hand side of the page now shows the number of matching datasets in each biome category (to save space, the 10 biome categories with the most matching datasets are shown by default).\n\n\n\n\nQuestion 25:\n\n\nWhich biome category has the most datasets that contain lactobacilli?\n\n\n\n\n\n\nQuestion 26:\n\n\nHow well does this correlate with what\u2019s known about these bacteria?\n\n\n\n\nFinally, we will try to use the interface to find functional proteins present under certain environmental conditions. For example, InterPro entry IPR001087 represents a domain found in GDSL esterases and lipases, which are hydrolytic enzymes with multifunctional properties.\n\n\n\n\nQuestion 27:\n\n\nUsing the search interface, can you identify the metagenomics datasets sampled from ocean sites at between 10 and 15 degrees C that contain these enzymes?\n\n\n\n\n\n\nQuestion 28:\n\n\nCan you envisage ways in which this kind of search functionality could be used for target / enzyme discovery?",
            "title": "Metagenomics Functional Analysis"
        },
        {
            "location": "/modules/metagenomics-module-fda/fda/#functional-data-analysis",
            "text": "",
            "title": "Functional Data Analysis"
        },
        {
            "location": "/modules/metagenomics-module-fda/fda/#key-learning-outcomes",
            "text": "After completing this module the trainee should be able to:    Understand how EMG provides functional analysis of metagenomic data sets    Know where to find and how to interpret analysis results for samples on the EMG website    Know how to download the raw sample data and analysis results for use with 3 rd  party visualisation and statistical analysis packages",
            "title": "Key Learning Outcomes"
        },
        {
            "location": "/modules/metagenomics-module-fda/fda/#resources-youll-be-using",
            "text": "",
            "title": "Resources You\u2019ll be Using"
        },
        {
            "location": "/modules/metagenomics-module-fda/fda/#tools-used",
            "text": "Megan6 :   http://ab.inf.uni-tuebingen.de/software/megan6/",
            "title": "Tools Used"
        },
        {
            "location": "/modules/metagenomics-module-fda/fda/#useful-links",
            "text": "EBI Metagenomics resource (EMG) :   www.ebi.ac.uk/metagenomics/",
            "title": "Useful Links"
        },
        {
            "location": "/modules/metagenomics-module-fda/fda/#introduction",
            "text": "The EBI Metagenomics resource (EMG) provides functional analysis of predicted coding sequences (pCDS) from metagenomic data sets using the InterPro database. InterPro is a sequence analysis resource that predicts protein family membership, along with the presence of important domains and sites. It does this by combining predictive models known as protein signatures from a number of different databases into a single searchable resource. InterPro curators manually integrate the different signatures, providing names and descriptive abstracts and, whenever possible, adding Gene Ontology (GO) terms. Links are also provided to pathway databases, such as KEGG, MetaCyc and Reactome, and to structural resources, such as SCOP, CATH and PDB.",
            "title": "Introduction"
        },
        {
            "location": "/modules/metagenomics-module-fda/fda/#what-are-protein-signatures",
            "text": "Protein signatures are obtained by modelling the conservation of amino acids at specific positions within a group of related proteins (i.e., a protein family), or within the domains/sites shared by a group of proteins. InterPro\u2019s different member databases use different computational methods to produce protein signatures, and they each have their own particular focus of interest: structural and/or functional domains, protein families, or protein features, such as active sites or binding sites (see Figure 1).   Figure 1. InterPro member databases grouped by the methods, indicated in white coloured text, used to construct their signatures. Their focus of interest is shown in blue text.  Only a subset of the InterPro member databases are used by EMG: Gene3D, TIGRFAMs, Pfam, PRINTS and PROSITE patterns. These databases were selected since, together, they provide both high coverage and offer detailed functional analysis, and have underlying algorithms that can cope with the vast amounts of fragmentary sequence data found in metagenomic datasets.",
            "title": "What are protein signatures?"
        },
        {
            "location": "/modules/metagenomics-module-fda/fda/#assigning-functional-information-to-metagenomic-sequences",
            "text": "Whilst InterPro matches to metagenomic sequence sets are informative in their own right, EMG offers an additional type of analysis in the form of Gene Ontology (GO) terms. The Gene Ontology is made up of 3 structured controlled vocabularies that describe gene products in terms of their associated biological processes, cellular components and molecular functions in a species-independent manner. By using GO terms, scientists working on different species or using different databases can compare datasets, since they have a precisely defined name and meaning\nfor a particular concept. Terms in the Gene Ontology are ordered into hierarchies, with less specific terms towards the top and more specific terms towards the bottom (see Figure 2).   Figure 2. An example of GO terms organised into a hierarchy, with terms becoming less specific as the hierarchy is ascended (e.g., alpha-tubulin binding is a type of cytoskeletal binding, which is a type of protein binding). Note that a GO term can have more than one parent term. The Gene Ontology also allows for different types of relationships between terms (such as \u2018has part of\u2019 or \u2018regulates\u2019). The EMG analysis pipeline only uses the straightforward \u2018is a\u2019 relationships.  More information about the GO project can be found http://www.geneontology.org/GO.doc.shtml  As part of the EMG analysis pipeline, GO terms for molecular function,biological process and cellular component are associated to pCDS in a sample via the InterPro2GO mapping service. This works as follows:\nInterPro entries are given GO terms by curators if the terms can be accurately applied to all of the proteins matching that entry. Sequences searched against InterPro are then associated with GO terms by virtue of the entries they match - a protein that matches one InterPro entry with the GO term \u2018kinase activity\u2019 and another InterPro entry with the GO term \u2018zinc ion binding\u2019 will be annotated with both GO terms.",
            "title": "Assigning functional information to metagenomic sequences"
        },
        {
            "location": "/modules/metagenomics-module-fda/fda/#finding-functional-information-about-samples-on-the-emg-website",
            "text": "Functional analysis of samples within projects on the EMG website www.ebi.ac.uk/metagenomics/  can be accessed by clicking on the Functional Analysis tab found toward the top of any sample page (see Figure 3 below).   Figure 3. A Functional analysis tab can be found towards the top of each run page  Clicking on this tab brings up a page displaying sequence features (the number of reads with pCDS, the number of pCDS with InterPro matches, etc), InterPro match information and GO term annotation for the sample, as shown in Figure 4 and 5 below.  InterPro match information for the predicted coding sequences in the sample is shown. The number of InterPro matches are displayed graphically, and as a table that has a text search facility.   Figure 4. Functional analysis of metagenomics data, as shown on the EMG website.   Figure 5. The GO terms predicted for the sample are displayed. Different graphical representations are available, and can be selected by clicking on the \u2018Switch view\u2019 icons.  The Gene Ontology terms displayed graphically on the web site have been \u2018slimmed\u2019 with a special GO slim developed for metagenomic data sets. GO slims are cut-down versions of the Gene Ontology, containing a subset of the terms in the whole GO. They give a broad overview of the ontology content without the detail of the specific fine-grained terms.  The full data sets used to generate both the InterPro and GO overview charts, along with a host of additional data (processed reads, pCDS, reads encoding 16S rRNAs, taxonomic analyses, etc) can be downloaded for further analysis by clicking the Download tab, found towards the top of the page (see Figure 6).   Figure 6. Each sample has a download tab, where the full set of sequences, analyses, summaries and raw data can be downloaded.",
            "title": "Finding functional information about samples on the EMG website"
        },
        {
            "location": "/modules/metagenomics-module-fda/fda/#practical",
            "text": "",
            "title": "Practical"
        },
        {
            "location": "/modules/metagenomics-module-fda/fda/#browsing-analysed-data-via-the-emg-website",
            "text": "For this session, we are going to look at the Ocean Sampling Day (OSD) 2014 project, which involved simultaneously sampling from geographically diverse oceanographic sites on Solstice 2014. A map of all of the sampling sites is shown on the project page:  https://www.ebi.ac.uk/metagenomics/projects/ERP009703  To get the OSD project page, either follow the above link or open the Metagenomics Portal home page ( https://www.ebi.ac.uk/metagenomics/ ).   enter \u2018 OSD \u2019 in the search box on the top right hand side of the page, and follow the link to project  ERP009703 . You should now have a Project overview page, which describes the project, submitter contact details, and links to the samples and runs that the project contains.  Scroll down to Associated Runs and use the \u2018 Filter \u2019 search box to find the OSD80_2014-06-21_0m_NPL022 sample ( ERS667582 ).   Click on the Sample Name link (not the Run link) to arrive at the overview page, describing various contextual data, such as the geographic location from which the material was isolated, its collection date, and so on.    Question 1:  What is the latitude, longitude and depth at which the sample was collected?    Question 2:  What geographic location does this correspond to?    Question 3:  What environmental ontology (ENVO) identifer and name has the sample material been annotated with?   Now scroll down to the \u2018Associated runs\u2019 section of the page. Some samples can have a number of sequencing runs associated with them (for example, corresponding to 16S rRNA amplicon analyses and WGS analyses performed on the same sample). In this case, there is only 1 associated run: ERR770971. Click on the Run ID to go to the Run page.  This page has a number of tabs towards the top (Overview, Quality control, Taxonomy analysis, Functional analysis, and Download). Click on the \u2018Download\u2019 tab. Click the file labelled \u2018 Predicted CDS \u2019 link to save this file to your computer. Find the file (it should be in your Downloads folder), unzip it and examine it using \u2018less\u2019 by typing the following commands in a terminal window:  Click on the \u2018Download\u2019 tab. Right click the file labelled \u2018Predicted CDS (FASTA)\u2019 link, and save this file to your desktop. Find the file, and either double click on it to open it, or examine it using \u2018less\u2019 by typing the following commands in a terminal window:  1\n2\n3 cd ~/Downloads\ngzip \u2013d ERR770971_MERGED_FASTQ_pCDS.faa.gz \nless ERR770971_MERGED_FASTQ_pCDS.faa   Have a look at one or two of the many sequences it contains.   You can count the total number of sequences in the file by grepping the number of header lines that start with \u201c>\u201d  1 grep -c  \"^> \" ERR770971_MERGED_FASTQ_pCDS.faa   In a moment, we will look at the analysis results for this entire batch of sequences, displayed on the EMG website. First, we will attempt to analyse just one of the predicted coding sequences using InterPro (the analysis results on the EMG website summarise these kind of results for hundreds of thousands of sequences).  In a new tab or window, open your web browser and navigate to  http://www.ebi.ac.uk/interpro/ . Copy and paste the following sequence into the text box on the InterPro home page where it says \u2018Analyse your sequence\u2019:  1\n2 >HWI-M02024:110:000000000-A8H0K:1:1101:23198:21331-1:N:0:TCAGAGAC_1_267\nHLLSYRYAYGKFSSTHEATIGGCFLTKDEELDDHIVKYEIWDTAGKNGTIHLPRCTTSKAYXIQVTWYRNAIAAVVVFDVTSRDSFEK   Press Search and wait for your results. Your sequence will be run through the InterProScan analysis software, which attempts to match it against all of the different signatures in the InterProScan database.   Question 4:  Which protein family does InterProScan predict your sequence belongs to, and what GO terms are predicted to describe its function?   Clicking on the InterPro entry name or IPR accession number will take you to the InterPro entry page for your result, where more information\ncan be found.  Return to the overview page for  ERR770971 .  Now we are going to look at the functional analysis results for all of the pCDS in the sample. First, we will find the number of sequences that made it through to the functional analysis section of the pipeline.  Click on the Quality control tab. This page displays a series of charts, showing how many sequences passed each quality control step, how many reads were left after clustering, and so on.   Question 5:  After all of the quality filtering steps are complete, how many reads were submitted for analysis by the pipeline?   Next, we will look at the results of the functional predictions for the\npCDS. These can be found under the Functional analysis tab.  Click on the Functional analysis tab and examine the InterPro match section. The top part of this page shows a sequence feature summary, showing the number of reads with predicted coding sequences (pCDS), the number of pCDS with InterPro matches, etc.   Question 6:  How many predicted coding sequences (pCDS) are in the run?    Question 7:  How many pCDS have InterProScan hits?   Scroll down the page to the InterPro match summary section   Question 8:  How many different InterPro entries were matched by the pCDS?    Question 9:  Why is this figure different to the number of pCDS that have InterProScan hits?   Next we will examine the GO terms predicted by InterPro for the pCDS in the sample.  Scroll down to the GO term annotation section of the page and examine the 3 bar charts, which correspond to the 3 different components of the\nGene Ontology.   Question 10:  What are the top 3 biological process terms predicted for the pCDS from the sample?   Selecting the pie chart representation of GO terms makes it easier to visualise the data to find the answer.  Now we will look at the taxonomic analysis for this run.  Click on the Taxonomy Analysis tab and examine the phylum composition graph and table.   Question 11:  How many of the WGS reads are predicted to encode 16S rRNAs?    Question 12:  What are the top 3 phyla in the run, according to 16S rRNA analysis?   Select the Krona chart view of the data icon. This brings up an\ninteractive chart that can be used to analyse data at different\ntaxonomic ranks.   Question 13:  What is the proportion of Polaribacter in the population?    Note  If the cyanobacteria section of the chart looks strange, this is because the version of GreenGenes used for analysis lists chloroplastic organisms under the cyanobacteria category; some of the cyanobacterial counts are, in fact, derived from photosynthetic eukaryotic organisms.   Now we will compare these analyses with those for a sample taken at 2 m depth from the same geographical location.  In a new tab or window, find and open the Ocean Sampling Day (OSD) 2014 project page again. Find the sample OSD80_2014-06-21_2m_NPL022 and examine metadata on the Overview page.   Question 14:  Other than sampling depth, what are the differences between this sample and OSD80_2014-06-21_0m_NPL02?   Scroll to the Assocated runs section, and click on ERR770970. Open the Functional analysis tab and examine the Sequence feature and InterPro match summary information for this run.   Question 15:  How many pCDS were in this run?     Question 16:  How many of the pCDSs have an InterPro match?    Question 17:  How many different InterPro entries are matched by this run?    Question 18:  Are these figures broadly comparable to those for the previous sample?   Now we are going to look at the differences in slimmed GO terms between the 2 runs. There are to ways to do this. First, you can simply scroll to the bottom of the page and examine the GO term annotation (note - selecting the bar chart representation of GO terms makes it easier to compare different data sets). Alternatively, you can use the comparison tool, which allows direct comparison of runs within a project. The tool can be accessed by clicking on the \u2018Comparison Tool\u2019 tab, illustrated in Figure 6 below. At present, the tool only compares slimmed GO terms, but will be expanded to cover full GO terms, InterPro annotations, and taxonomic profiles as development of the site continues.  Click on the Comparison tool tab and choose the Ocean Sampling Day (OSD) 2014 project from the sample list and select the OSD80_2014-06- 21_0m_NPL022 - ERR770971 and OSD80_2014-06-21_2m_NPL022 - ERR770970.   Question 19:  Are there visible differences between the GO terms for these runs. Could there be any biological explanation for this?   Navigate back and open the taxonomic analysis results tab for each run.   Question 20:  How does the taxonomic composition differ between runs? Are any trends in the data consistent with your answer to question 19?",
            "title": "Browsing analysed data via the EMG website"
        },
        {
            "location": "/modules/metagenomics-module-fda/fda/#visualising-taxonomic-data-using-megan",
            "text": "Next, we are going to look at the taxonomic predictions for all of the runs. To do this, we are going to load them into MEGAN. MEGAN is a tool suite that provides metagenomic data analysis and visualization. We are going to use only a small subset of its features, relating to taxonomic comparison. Detailed information on MEGAN and the analyses and visualisations it offers can be found here:  1 http://ab.inf.uni-tuebingen.de/data/software/megan5/download/manual.pdf   MEGAN can be downloaded from  1 http://ab.inf.uni-tuebingen.de/software/megan6/  \n(there are 2 editions Community and Ultimate. The Community edition is open source and free to download. We will use this version for the tutorial). However, it should already be installed on your Desktop.  Click  on the MEGAN icon on your desktop to load the s/w.  We now need to download the full taxonomic predictions for all of the runs in the Ocean Sampling Day project.  Navigate to the Project: Ocean Sampling Day (OSD) page, using the breadcrumb URL link at the top of the page. Click on the Analysis summary tab, which will take you to a set of tab separated result matrix files, summarising the taxonomic and functional observations for all runs in the project.  Click on the Taxonomic assignments (TSV) link, which will download the corresponding file to your computer. When prompted, choose to save the file in the Downloads folder.  Open the Terminal, navigate to the Downloads directory and take a look at the file you have just downloaded (hit q to exit less):  1\n2 cd  ~/Downloads\nless -S ERP009703_taxonomy_abundances_v2.0.tsv   You will see it is a large matrix file, with abundance counts for each taxonomic lineage for each run in the project. From the MEGAN menu, choose \u2018File\u2019 and \u2018Import\u2019. Select \u2018CSV Import\u2019 and then find the file you have just downloaded and edited.  From the pop up menu, go with the default options in the Format and Separator sections (which should have the \u2018Class, Count\u2019 option selected under format, and the separator set as \u2018Tab\u2019) and select \u2018Taxonomy\u2019 in classification section, then press \u2018Apply\u2019.  There are many different visualisations and comparisons that can be performed using MEGAN, so feel free to explore the data using the tool.  Click on the \u2018Show chart\u2019 icon and choose \u2018Stacked Bar Chart\u2019. This should give you a bar chart, showing the abundance reads for taxonomic lineages across all of the sampling sites.   Question 21:  Are the number of classified 16S reads roughly equivalent across all of the different sampling sites?    Question 22:  Which run appears particularly enriched in Polaribacter?   You can change between abundance counts and relative abundance using the Options drop menu and choosing % Percentage Scale or Linear Scale.  Change the chart view to \u2018Bubble Chart\u2019. This visualisation can be useful when comparing a large number of samples.   Question 23:  Do any samples contain taxa not found in other samples?    Take a look at the Schlegelella genus.   Question 24:  Can you discern any patterns in the geographical distribution of certain species (for example, the cluster of samples enriched for the lactobacillus genus compared to other samples)?   We are now going to take a look at which other datasets in EBI Metagenomics that lactobacilli are found in. Point your browser at  https://www.ebi.ac.uk/metagenomics/search/  This interface allows you to search the project, sample and run related metadata and analysis results for all of the publicly available datasets in the EBI Metagenomics resource.  Click on the \u2018Runs\u2019 tab. You should now see a number of run-related metadata search facets on the left hand side of the page, including \u2018Organism\u2019. Click on the \u2018More\u2026\u2019 option under Organism, scroll down to \u2018Lactobacillus\u2019 in the pop-up window and select the check box next to it. Now click \u2018Filter\u2019. The results page should now show all of the runs that have taxonomic matches to lactobacilli in their datasets. The \u2018Biome\u2019 facet on the left hand side of the page now shows the number of matching datasets in each biome category (to save space, the 10 biome categories with the most matching datasets are shown by default).   Question 25:  Which biome category has the most datasets that contain lactobacilli?    Question 26:  How well does this correlate with what\u2019s known about these bacteria?   Finally, we will try to use the interface to find functional proteins present under certain environmental conditions. For example, InterPro entry IPR001087 represents a domain found in GDSL esterases and lipases, which are hydrolytic enzymes with multifunctional properties.   Question 27:  Using the search interface, can you identify the metagenomics datasets sampled from ocean sites at between 10 and 15 degrees C that contain these enzymes?    Question 28:  Can you envisage ways in which this kind of search functionality could be used for target / enzyme discovery?",
            "title": "Visualising taxonomic data using MEGAN"
        }
    ]
}